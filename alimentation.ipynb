{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a summary of the dj columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## Create Summary column for the data ################\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Synthetize the alimentation\n",
    "data = pd.read_csv('sncb_data_challenge.csv', sep=';')\n",
    "\n",
    "\n",
    "def str_to_bool_list(string):\n",
    "    # convert string to list of boolean\n",
    "    if pd.isna(string):\n",
    "        return []\n",
    "    return [s.strip() in 'True' for s in string.strip('[]').split(',')]\n",
    "\n",
    "# Convert string to list of boolean\n",
    "col_list_bool = ['dj_ac_state_sequence', 'dj_dc_state_sequence']\n",
    "\n",
    "for col in col_list_bool:\n",
    "    data[col] = data[col].apply(str_to_bool_list)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['events_sequence'] = data['events_sequence'].apply(lambda x: list(map(int, x.strip('[]').split(','))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary column of the alimentation that will be had to the model\n",
    "data['summary'] = None\n",
    "for i in range(len(data[col_list_bool[0]])):\n",
    "    summary_list = []\n",
    "    for j in range(len(data[col_list_bool[0]][i])):\n",
    "        if data[col_list_bool[0]][i][j] and data[col_list_bool[1]][i][j]:\n",
    "            summary_list.append('AC/DC')\n",
    "        if data[col_list_bool[0]][i][j] and not data[col_list_bool[1]][i][j]:\n",
    "            summary_list.append('AC')\n",
    "        elif not data[col_list_bool[0]][i][j] and data[col_list_bool[1]][i][j]:\n",
    "            summary_list.append('DC')\n",
    "        else:\n",
    "            summary_list.append('Battery')\n",
    "    data.at[i, 'summary'] = summary_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Work on the sequences of  the summary alone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determination of the sequence of the alimentation alone\n",
    "data['summary_alone'] = None\n",
    "for i in range(len(data['summary'])):\n",
    "    new_summary_list = []\n",
    "    for j in range(len(data['summary'][i])):\n",
    "        if j==0 or data['summary'][i][j] != data['summary'][i][j-1]:\n",
    "            new_summary_list.append(data['summary'][i][j])\n",
    "    data.at[i, 'summary_alone'] = new_summary_list\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incident Type: 4\n",
      "    support       itemsets\n",
      "0  0.961538           [DC]\n",
      "1  0.858974      [Battery]\n",
      "2  0.820513  [DC, Battery]\n",
      "Incident Type: 13\n",
      "    support       itemsets\n",
      "0  0.977987           [DC]\n",
      "1  0.672956      [Battery]\n",
      "2  0.654088  [DC, Battery]\n",
      "Incident Type: 14\n",
      "    support       itemsets\n",
      "0  0.966443      [Battery]\n",
      "1  0.852349           [DC]\n",
      "3  0.818792  [DC, Battery]\n",
      "2  0.382550           [AC]\n",
      "4  0.382550  [Battery, AC]\n",
      "Incident Type: 2\n",
      "    support       itemsets\n",
      "0  0.991597           [DC]\n",
      "1  0.663866      [Battery]\n",
      "2  0.655462  [DC, Battery]\n",
      "Incident Type: 11\n",
      "    support       itemsets\n",
      "0  0.923077           [DC]\n",
      "1  0.884615      [Battery]\n",
      "2  0.807692  [DC, Battery]\n",
      "Incident Type: 99\n",
      "    support       itemsets\n",
      "0  0.954286           [DC]\n",
      "1  0.680000      [Battery]\n",
      "2  0.634286  [DC, Battery]\n",
      "Incident Type: 9\n",
      "    support       itemsets\n",
      "0  1.000000           [DC]\n",
      "1  0.470085      [Battery]\n",
      "2  0.470085  [DC, Battery]\n",
      "Incident Type: 17\n",
      "   support       itemsets\n",
      "0      0.8      [Battery]\n",
      "1      0.8           [DC]\n",
      "2      0.6  [DC, Battery]\n",
      "Incident Type: 3\n",
      "   support       itemsets\n",
      "0      0.8           [DC]\n",
      "1      0.6      [Battery]\n",
      "2      0.4  [DC, Battery]\n",
      "Incident Type: 16\n",
      "   support       itemsets\n",
      "0     1.00           [DC]\n",
      "1     0.75      [Battery]\n",
      "2     0.75  [DC, Battery]\n",
      "Incident Type: 6\n",
      "    support       itemsets\n",
      "0  1.000000      [Battery]\n",
      "1  0.833333           [DC]\n",
      "2  0.833333  [DC, Battery]\n",
      "Incident Type: 7\n",
      "   support       itemsets\n",
      "0     1.00           [DC]\n",
      "1     0.75      [Battery]\n",
      "2     0.75  [DC, Battery]\n"
     ]
    }
   ],
   "source": [
    "############################# Frequent itemsets (FP-Growth) #############################\n",
    "\n",
    "import pandas as pd\n",
    "from mlxtend.frequent_patterns import fpgrowth\n",
    "import ast  # For safely evaluating string representations of lists\n",
    "\n",
    "\n",
    "def find_frequent_itemsets_fp_growth(data, min_support=0.3):\n",
    "    \"\"\"\n",
    "    Finds the most frequent sequences of events for each incident type using FP-Growth.\n",
    "    \"\"\"\n",
    "    # Get all unique incident types\n",
    "    incident_types = data['incident_type'].unique()\n",
    "    results = {}\n",
    "\n",
    "    for incident in incident_types:\n",
    "        # Check if the csv file already exists\n",
    "        try:\n",
    "            most_frequent = pd.read_csv(f'results\\\\results2\\\\results2_{incident}.csv', sep=';')\n",
    "            if most_frequent is not None:\n",
    "                results[incident] = most_frequent\n",
    "                continue\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # Filter rows for the current incident type\n",
    "        filtered_data = data[data['incident_type'] == incident]\n",
    "\n",
    "        # Prepare transactions: each transaction is a sequence of events\n",
    "        transactions = filtered_data['summary_alone']\n",
    "\n",
    "        # Create a one-hot encoded DataFrame for the events\n",
    "        unique_events = set(alimentation for sequence in transactions for alimentation in sequence)  # All unique events\n",
    "        transaction_df = pd.DataFrame([\n",
    "            {event: (event in sequence) for event in unique_events} for sequence in transactions\n",
    "        ])\n",
    "        # Apply FP-Growth algorithm\n",
    "        frequent_itemsets = fpgrowth(transaction_df, min_support=min_support, use_colnames=True)\n",
    "\n",
    "        # Sort by support and keep top results\n",
    "        if not frequent_itemsets.empty:\n",
    "            most_frequent = frequent_itemsets.sort_values(by='support', ascending=False)\n",
    "            most_frequent['itemsets'] = frequent_itemsets['itemsets'].apply(lambda x: list(x))\n",
    "            results[incident] = most_frequent\n",
    "        else:\n",
    "            results[incident] = None\n",
    "        \n",
    "        # store the results in a csv file\n",
    "        most_frequent.to_csv(f'results\\\\results2\\\\results2_{incident}.csv', sep=';', index=False)\n",
    "    # Run for all the database\n",
    "    transactions = data['summary_alone']\n",
    "    unique_events = set(event for sequence in transactions for event in sequence)  # All unique events\n",
    "    transaction_df = pd.DataFrame([\n",
    "        {event: (event in sequence) for event in unique_events} for sequence in transactions\n",
    "    ])\n",
    "    database_frequent_itemsets = fpgrowth(transaction_df, min_support=min_support, use_colnames=True)\n",
    "    database_frequent_itemsets = database_frequent_itemsets.sort_values(by='support', ascending=False)\n",
    "    database_frequent_itemsets['itemsets'] = database_frequent_itemsets['itemsets'].apply(lambda x: list(x))\n",
    "    database_frequent_itemsets.to_csv(f'results\\\\results2\\\\results_database2.csv', sep=';', index=False)\n",
    "    return results\n",
    "\n",
    "# Run the function\n",
    "results = find_frequent_itemsets_fp_growth(data)\n",
    "\n",
    "# Display the results\n",
    "for incident, frequent in results.items():\n",
    "    print(f\"Incident Type: {incident}\")\n",
    "    if frequent is not None:\n",
    "        print(frequent)\n",
    "    else:\n",
    "        print(\"No frequent sequences found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Work on a the combination between event and the summary colomn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the most frequent sequence of alimentation combined with the event column\n",
    "\n",
    "data['events + summary'] = None\n",
    "for i in range(len(data['summary'])):\n",
    "    ev_sum_list = []\n",
    "    for j in range(len(data['summary'][i])):\n",
    "        ev_sum_list.append((data['events_sequence'][i][j], data['summary'][i][j]))\n",
    "    data.at[i, 'events + summary'] = ev_sum_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of duplicates in the 'events + summary' column\n",
    "\n",
    "for i in range(len(data['events + summary'])):\n",
    "    new_ev_sum_list = []\n",
    "    for j in range(len(data['events + summary'][i])):\n",
    "        if j == 0 or data['events + summary'][i][j] != data['events + summary'][i][j-1]:\n",
    "            new_ev_sum_list.append(data['events + summary'][i][j])\n",
    "    data.at[i, 'events + summary'] = new_ev_sum_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       [(2744, Battery), (4004, Battery), (2852, Batt...\n",
      "1       [(2744, DC), (4148, DC), (4394, DC), (1566, DC...\n",
      "2       [(4394, DC), (1566, DC), (1570, DC), (4114, DC...\n",
      "3       [(4066, DC), (4068, DC), (2742, DC), (4026, DC...\n",
      "4       [(4002, Battery), (4032, Battery), (4028, Batt...\n",
      "                              ...                        \n",
      "1006    [(4002, Battery), (2852, Battery), (4110, Batt...\n",
      "1007    [(3490, DC), (4068, DC), (4066, DC), (4068, DC...\n",
      "1008    [(4066, DC), (4068, DC), (3658, DC), (4066, DC...\n",
      "1009    [(2956, AC), (4068, AC), (3636, AC), (3658, AC...\n",
      "1010    [(4004, Battery), (4032, Battery), (4028, Batt...\n",
      "Name: events + summary, Length: 1011, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(data['events + summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incident Type: 4\n",
      "    support                  itemsets\n",
      "0  0.948718              [(2708, DC)]\n",
      "1  0.910256              [(4026, DC)]\n",
      "2  0.910256              [(4066, DC)]\n",
      "3  0.910256  [(2708, DC), (4026, DC)]\n",
      "4  0.910256  [(2708, DC), (4066, DC)]\n",
      "Incident Type: 13\n",
      "    support      itemsets\n",
      "0  0.924528  [(4026, DC)]\n",
      "1  0.924528  [(2708, DC)]\n",
      "2  0.915094  [(4066, DC)]\n",
      "3  0.908805  [(4068, DC)]\n",
      "Incident Type: 14\n",
      "    support           itemsets\n",
      "0  0.939597  [(4140, Battery)]\n",
      "Incident Type: 2\n",
      "      support                                           itemsets\n",
      "0    0.974790                                       [(2708, DC)]\n",
      "1    0.974790                                       [(4066, DC)]\n",
      "2    0.966387                                       [(4026, DC)]\n",
      "13   0.966387                           [(2708, DC), (4066, DC)]\n",
      "14   0.966387                           [(2708, DC), (4026, DC)]\n",
      "..        ...                                                ...\n",
      "145  0.907563  [(2708, DC), (4026, DC), (4066, DC), (2742, DC...\n",
      "144  0.907563   [(3658, DC), (4066, DC), (2742, DC), (4026, DC)]\n",
      "143  0.907563   [(2708, DC), (3658, DC), (2742, DC), (4026, DC)]\n",
      "141  0.907563               [(3658, DC), (4120, DC), (2742, DC)]\n",
      "214  0.907563               [(2708, DC), (4066, DC), (2956, DC)]\n",
      "\n",
      "[215 rows x 2 columns]\n",
      "Incident Type: 11\n",
      "    support      itemsets\n",
      "0  0.923077  [(2708, DC)]\n",
      "Incident Type: 99\n",
      "No frequent sequences found.\n",
      "Incident Type: 9\n",
      "     support                                           itemsets\n",
      "0   1.000000                                       [(2708, DC)]\n",
      "6   1.000000                           [(2708, DC), (4066, DC)]\n",
      "8   1.000000                           [(2708, DC), (4068, DC)]\n",
      "7   1.000000                           [(4068, DC), (4066, DC)]\n",
      "9   1.000000               [(2708, DC), (4068, DC), (4066, DC)]\n",
      "..       ...                                                ...\n",
      "43  0.957265               [(3658, DC), (4066, DC), (4026, DC)]\n",
      "42  0.957265               [(3636, DC), (4066, DC), (4026, DC)]\n",
      "40  0.957265               [(3658, DC), (4068, DC), (4026, DC)]\n",
      "39  0.957265               [(3636, DC), (4068, DC), (4026, DC)]\n",
      "62  0.957265  [(2708, DC), (4026, DC), (4066, DC), (4068, DC...\n",
      "\n",
      "[63 rows x 2 columns]\n",
      "Incident Type: 17\n",
      "No frequent sequences found.\n",
      "Incident Type: 3\n",
      "No frequent sequences found.\n",
      "Incident Type: 16\n",
      "      support                                           itemsets\n",
      "0         1.0                                       [(3658, DC)]\n",
      "687       1.0  [(2708, DC), (4092, DC), (4124, DC), (3636, DC...\n",
      "674       1.0  [(2708, DC), (4092, DC), (4068, DC), (4124, DC...\n",
      "675       1.0  [(4092, DC), (4068, DC), (4124, DC), (3636, DC...\n",
      "676       1.0  [(4092, DC), (4066, DC), (4068, DC), (4124, DC...\n",
      "...       ...                                                ...\n",
      "346       1.0   [(2708, DC), (2744, DC), (4066, DC), (2956, DC)]\n",
      "347       1.0   [(2744, DC), (4066, DC), (2970, DC), (2956, DC)]\n",
      "348       1.0   [(2708, DC), (4066, DC), (2970, DC), (2956, DC)]\n",
      "349       1.0   [(2708, DC), (2744, DC), (2970, DC), (2956, DC)]\n",
      "1022      1.0  [(2708, DC), (4092, DC), (4066, DC), (4068, DC...\n",
      "\n",
      "[1023 rows x 2 columns]\n",
      "Incident Type: 6\n",
      "    support                                           itemsets\n",
      "0       1.0                                  [(4002, Battery)]\n",
      "47      1.0  [(2852, Battery), (4026, Battery), (4002, Batt...\n",
      "34      1.0  [(4026, Battery), (2708, Battery), (2852, Batt...\n",
      "35      1.0  [(2854, Battery), (2708, Battery), (2852, Batt...\n",
      "36      1.0  [(4026, Battery), (2854, Battery), (2708, Batt...\n",
      "..      ...                                                ...\n",
      "26      1.0  [(4002, Battery), (4026, Battery), (4110, Batt...\n",
      "27      1.0  [(4002, Battery), (2854, Battery), (4110, Batt...\n",
      "28      1.0  [(4026, Battery), (4002, Battery), (2852, Batt...\n",
      "29      1.0  [(4002, Battery), (2854, Battery), (2852, Batt...\n",
      "62      1.0  [(4002, Battery), (2852, Battery), (4110, Batt...\n",
      "\n",
      "[63 rows x 2 columns]\n",
      "Incident Type: 7\n",
      "      support                                           itemsets\n",
      "0         1.0                                       [(3658, DC)]\n",
      "1360      1.0  [(2708, DC), (2682, DC), (4026, DC), (4078, DC...\n",
      "1373      1.0  [(2708, DC), (2682, DC), (4066, DC), (4078, DC...\n",
      "1372      1.0  [(2682, DC), (4066, DC), (4078, DC), (4068, DC...\n",
      "1371      1.0  [(2708, DC), (2682, DC), (4066, DC), (4068, DC...\n",
      "...       ...                                                ...\n",
      "677       1.0  [(4026, DC), (4066, DC), (4078, DC), (4124, DC...\n",
      "676       1.0  [(2708, DC), (4026, DC), (4066, DC), (4124, DC...\n",
      "675       1.0  [(4026, DC), (4066, DC), (4124, DC), (3658, DC...\n",
      "674       1.0  [(4026, DC), (4078, DC), (4124, DC), (3636, DC...\n",
      "2046      1.0  [(2708, DC), (2682, DC), (4026, DC), (4066, DC...\n",
      "\n",
      "[2047 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "########################### Frequent itemsets (FP-Growth) #############################\n",
    "\n",
    "def find_frequent_itemsets_fp_growth(data, min_support=0.9):\n",
    "    \"\"\"\n",
    "    Finds the most frequent sequences of events for each incident type using FP-Growth.\n",
    "    \"\"\"\n",
    "    # Get all unique incident types\n",
    "    incident_types = data['incident_type'].unique()\n",
    "    results = {}\n",
    "\n",
    "    for incident in incident_types:\n",
    "        # Check if the csv file already exists\n",
    "        try:\n",
    "            most_frequent = pd.read_csv(f'results\\\\results3\\\\results3_{incident}.csv', sep=';')\n",
    "            if most_frequent is not None:\n",
    "                results[incident] = most_frequent\n",
    "                continue\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # Filter rows for the current incident type\n",
    "        filtered_data = data[data['incident_type'] == incident]\n",
    "\n",
    "        # Prepare transactions: each transaction is a sequence of events\n",
    "        transactions = filtered_data['events + summary']\n",
    "\n",
    "        # Create a one-hot encoded DataFrame for the events\n",
    "        unique_events = set((event, summary) for sequence in transactions for event, summary in sequence)  # All unique events\n",
    "        transaction_df = pd.DataFrame([\n",
    "            {(event, summary): (event, summary) in sequence for event, summary in unique_events} for sequence in transactions\n",
    "        ])\n",
    "        # Apply FP-Growth algorithm\n",
    "        frequent_itemsets = fpgrowth(transaction_df, min_support=min_support, use_colnames=True)\n",
    "\n",
    "        # Sort by support and keep top results\n",
    "        if not frequent_itemsets.empty:\n",
    "            most_frequent = frequent_itemsets.sort_values(by='support', ascending=False)\n",
    "            most_frequent['itemsets'] = frequent_itemsets['itemsets'].apply(lambda x: list(x))\n",
    "            results[incident] = most_frequent\n",
    "        else:\n",
    "            results[incident] = None\n",
    "        \n",
    "        # store the results in a csv file\n",
    "        most_frequent.to_csv(f'results\\\\results3\\\\results3_{incident}.csv', sep=';', index=False)\n",
    "    # Run for all the database\n",
    "    transactions = data['events + summary']\n",
    "    unique_events = set((event, summary) for sequence in transactions for event, summary in sequence)  # All unique events\n",
    "    transaction_df = pd.DataFrame([\n",
    "        {(event, summary): (event, summary) in sequence for event, summary in unique_events} for sequence in transactions\n",
    "    ])\n",
    "    database_frequent_itemsets = fpgrowth(transaction_df, min_support=min_support, use_colnames=True)\n",
    "    database_frequent_itemsets = database_frequent_itemsets.sort_values(by='support', ascending=False)\n",
    "    database_frequent_itemsets['itemsets'] = database_frequent_itemsets['itemsets'].apply(lambda x: list(x))\n",
    "    database_frequent_itemsets.to_csv(f'results\\\\results3\\\\results_database3.csv', sep=';', index=False)\n",
    "    return results\n",
    "\n",
    "# Run the function\n",
    "results = find_frequent_itemsets_fp_growth(data)\n",
    "\n",
    "# Display the results\n",
    "for incident, frequent in results.items():\n",
    "    print(f\"Incident Type: {incident}\")\n",
    "    if frequent is not None:\n",
    "        print(frequent)\n",
    "    else:\n",
    "        print(\"No frequent sequences found.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'sncb_alimentation.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Save the new alimentation\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m data\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msncb_alimentation.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\bryan\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:3772\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3761\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[0;32m   3763\u001b[0m formatter \u001b[38;5;241m=\u001b[39m DataFrameFormatter(\n\u001b[0;32m   3764\u001b[0m     frame\u001b[38;5;241m=\u001b[39mdf,\n\u001b[0;32m   3765\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3769\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[0;32m   3770\u001b[0m )\n\u001b[1;32m-> 3772\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrameRenderer(formatter)\u001b[38;5;241m.\u001b[39mto_csv(\n\u001b[0;32m   3773\u001b[0m     path_or_buf,\n\u001b[0;32m   3774\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[0;32m   3775\u001b[0m     sep\u001b[38;5;241m=\u001b[39msep,\n\u001b[0;32m   3776\u001b[0m     encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[0;32m   3777\u001b[0m     errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m   3778\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[0;32m   3779\u001b[0m     quoting\u001b[38;5;241m=\u001b[39mquoting,\n\u001b[0;32m   3780\u001b[0m     columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m   3781\u001b[0m     index_label\u001b[38;5;241m=\u001b[39mindex_label,\n\u001b[0;32m   3782\u001b[0m     mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[0;32m   3783\u001b[0m     chunksize\u001b[38;5;241m=\u001b[39mchunksize,\n\u001b[0;32m   3784\u001b[0m     quotechar\u001b[38;5;241m=\u001b[39mquotechar,\n\u001b[0;32m   3785\u001b[0m     date_format\u001b[38;5;241m=\u001b[39mdate_format,\n\u001b[0;32m   3786\u001b[0m     doublequote\u001b[38;5;241m=\u001b[39mdoublequote,\n\u001b[0;32m   3787\u001b[0m     escapechar\u001b[38;5;241m=\u001b[39mescapechar,\n\u001b[0;32m   3788\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m   3789\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\bryan\\anaconda3\\Lib\\site-packages\\pandas\\io\\formats\\format.py:1186\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m   1165\u001b[0m     created_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1167\u001b[0m csv_formatter \u001b[38;5;241m=\u001b[39m CSVFormatter(\n\u001b[0;32m   1168\u001b[0m     path_or_buf\u001b[38;5;241m=\u001b[39mpath_or_buf,\n\u001b[0;32m   1169\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1184\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt,\n\u001b[0;32m   1185\u001b[0m )\n\u001b[1;32m-> 1186\u001b[0m csv_formatter\u001b[38;5;241m.\u001b[39msave()\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[0;32m   1189\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[1;32mc:\\Users\\bryan\\anaconda3\\Lib\\site-packages\\pandas\\io\\formats\\csvs.py:240\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;124;03mCreate the writer & save.\u001b[39;00m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[1;32m--> 240\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_handle(\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilepath_or_buffer,\n\u001b[0;32m    242\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    243\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    244\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merrors,\n\u001b[0;32m    245\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompression,\n\u001b[0;32m    246\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstorage_options,\n\u001b[0;32m    247\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m csvlib\u001b[38;5;241m.\u001b[39mwriter(\n\u001b[0;32m    250\u001b[0m         handles\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[0;32m    251\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    256\u001b[0m         quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotechar,\n\u001b[0;32m    257\u001b[0m     )\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save()\n",
      "File \u001b[1;32mc:\\Users\\bryan\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    854\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    855\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    856\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    857\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    858\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    860\u001b[0m             handle,\n\u001b[0;32m    861\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    862\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    863\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    864\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    865\u001b[0m         )\n\u001b[0;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'sncb_alimentation.csv'"
     ]
    }
   ],
   "source": [
    "# Save the new alimentation\n",
    "data.to_csv('sncb_alimentation.csv', sep=';', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
