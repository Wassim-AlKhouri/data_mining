{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preperation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################# Data preparation script for the SNCB Data Challenge #############################\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "MIN_BEFORE_INCIDENT = 30\n",
    "MIN_AFTER_INCIDENT = 15\n",
    "\n",
    "USE_ITEMSETS = True\n",
    "   \n",
    "# Read CSV file\n",
    "data = pd.read_csv('sncb_data_challenge.csv', sep=';')\n",
    "\n",
    "# Convert string to list of integers\n",
    "col_list_int = ['vehicles_sequence', 'events_sequence','seconds_to_incident_sequence']\n",
    "for col in col_list_int:\n",
    "    data[col] = data[col].apply(lambda x: list(map(int, x.strip('[]').split(','))))\n",
    "\n",
    "# Convert string to list of floats\n",
    "col_list_float = ['train_kph_sequence']\n",
    "for col in col_list_float:\n",
    "    data[col] = data[col].apply(lambda x: list(map(float, x.strip('[]').split(','))))\n",
    "\n",
    "def str_to_bool_list(string):\n",
    "    # convert string to list of boolean\n",
    "    if pd.isna(string):\n",
    "        return []\n",
    "    return [s.strip() in 'True' for s in string.strip('[]').split(',')]\n",
    " \n",
    "# Convert string to list of boolean\n",
    "col_list_bool = ['dj_ac_state_sequence', 'dj_dc_state_sequence']\n",
    " \n",
    "for col in col_list_bool:\n",
    "    data[col] = data[col].apply(str_to_bool_list)\n",
    "\n",
    "# Create a new column for the duration of each event\n",
    "data['duration_sequence'] = [[] for _ in range(len(data))]\n",
    "\n",
    "# Compute the acceleration\n",
    "data['acceleration_seq'] = data.apply(\n",
    "    lambda row: [\n",
    "        (row['train_kph_sequence'][i + 1] - row['train_kph_sequence'][i]) / \n",
    "        (row['seconds_to_incident_sequence'][i + 1] - row['seconds_to_incident_sequence'][i])\n",
    "        if (row['seconds_to_incident_sequence'][i + 1] - row['seconds_to_incident_sequence'][i]) != 0 and row['vehicles_sequence'][i+1] == row['vehicles_sequence'][i] else np.nan\n",
    "        for i in range(len(row['train_kph_sequence']) - 1)\n",
    "    ], axis=1)\n",
    "\n",
    "# Remove redundant events\n",
    "for i in range(len(data['events_sequence'])):\n",
    "    new_vehicles_sequence = []\n",
    "    new_events_sequence = []\n",
    "    new_seconds_to_incident_sequence = []\n",
    "    new_train_kph_sequence = []\n",
    "    new_dj_ac_state_sequence = []\n",
    "    new_dj_dc_state_sequence = []\n",
    "    new_acceleration_seq = []\n",
    "    duration_sequence = []\n",
    "    \n",
    "    start_index = 0\n",
    "    # duration = 0\n",
    "    for j in range(len(data['events_sequence'][i]) - 1):\n",
    "        is_before_incident = data['seconds_to_incident_sequence'][i][j] >= -MIN_BEFORE_INCIDENT * 60\n",
    "        is_after_incident = data['seconds_to_incident_sequence'][i][j] <= MIN_AFTER_INCIDENT * 60\n",
    "        time_condition = is_before_incident and is_after_incident\n",
    "        # Skip row that are not in the time window\n",
    "        if not time_condition:\n",
    "            start_index = j+1\n",
    "            continue\n",
    "        \n",
    "        # Keep every event that is not the same as the previous one\n",
    "        if j == start_index or data['events_sequence'][i][j] != new_events_sequence[-1]:\n",
    "            new_vehicles_sequence.append(data['vehicles_sequence'][i][j])\n",
    "            new_events_sequence.append(data['events_sequence'][i][j])\n",
    "            new_seconds_to_incident_sequence.append(data['seconds_to_incident_sequence'][i][j])\n",
    "            new_train_kph_sequence.append(data['train_kph_sequence'][i][j])\n",
    "            new_dj_ac_state_sequence.append(data['dj_ac_state_sequence'][i][j])\n",
    "            new_dj_dc_state_sequence.append(data['dj_dc_state_sequence'][i][j])\n",
    "            if j < len(data['acceleration_seq'][i]):\n",
    "                new_acceleration_seq.append(data['acceleration_seq'][i][j])\n",
    "            duration_sequence.append(0)\n",
    "\n",
    "        # Compute the duration of the event\n",
    "        elif data['events_sequence'][i][j] == new_events_sequence[-1]:\n",
    "            duration_sequence[-1] += abs(data['seconds_to_incident_sequence'][i][j] - data['seconds_to_incident_sequence'][i][j-1])\n",
    "\n",
    "    \n",
    "        # Add the last duration\n",
    "        duration_sequence[-1] += abs(data['seconds_to_incident_sequence'][i][j+1] - data['seconds_to_incident_sequence'][i][j])        \n",
    "    if len(new_events_sequence) == 0:\n",
    "        continue\n",
    "    \n",
    "    data.at[i, 'vehicles_sequence'] = new_vehicles_sequence\n",
    "    data.at[i, 'events_sequence'] = new_events_sequence\n",
    "    data.at[i, 'seconds_to_incident_sequence'] = new_seconds_to_incident_sequence\n",
    "    data.at[i, 'train_kph_sequence'] = new_train_kph_sequence\n",
    "    data.at[i, 'dj_ac_state_sequence'] = new_dj_ac_state_sequence\n",
    "    data.at[i, 'dj_dc_state_sequence'] = new_dj_dc_state_sequence\n",
    "    data.at[i, 'acceleration_seq'] = new_acceleration_seq\n",
    "    data.at[i, 'duration_sequence'] = duration_sequence\n",
    "\n",
    "# for i in range(len(data['events_sequence'])):\n",
    "#     if len(data['events_sequence'][i]) == 0:\n",
    "#         print(\"empty\")\n",
    "    \n",
    "# Save the modified DataFrame to a new CSV file\n",
    "data.to_csv('sncb_prepared.csv', sep=';', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Finding frequent itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################# Frequent itemsets (FP-Growth) #############################\n",
    "if USE_ITEMSETS:\n",
    "    import pandas as pd\n",
    "    from mlxtend.frequent_patterns import fpgrowth\n",
    "    import ast  # For safely evaluating string representations of lists\n",
    "\n",
    "    MIN_SUPPORT = 0.3\n",
    "\n",
    "    def find_frequent_itemsets_fp_growth(data, min_support=MIN_SUPPORT):\n",
    "        \"\"\"\n",
    "        Finds the most frequent sequences of events for each incident type using FP-Growth.\n",
    "        \"\"\"\n",
    "        # Get all unique incident types\n",
    "        incident_types = data['incident_type'].unique()\n",
    "        results = {}\n",
    "\n",
    "        # Convert stringified lists to actual lists of integers\n",
    "        data['events_sequence'] = data['events_sequence'].apply(lambda x: ast.literal_eval(x))\n",
    "\n",
    "        for incident in incident_types:\n",
    "            # Check if the csv file already exists\n",
    "            # try:\n",
    "            #     most_frequent = pd.read_csv(f'results\\\\results_{incident}.csv', sep=';')\n",
    "            #     if most_frequent is not None:\n",
    "            #         results[incident] = most_frequent\n",
    "            #         continue\n",
    "            # except:\n",
    "            #     pass\n",
    "\n",
    "            # Filter rows for the current incident type\n",
    "            filtered_data = data[data['incident_type'] == incident]\n",
    "\n",
    "            # Prepare transactions: each transaction is a sequence of events\n",
    "            transactions = filtered_data['events_sequence'].tolist()\n",
    "\n",
    "            # Create a one-hot encoded DataFrame for the events\n",
    "            unique_events = set(event for sequence in transactions for event in sequence)  # All unique events\n",
    "            transaction_df = pd.DataFrame([\n",
    "                {event: (event in sequence) for event in unique_events} for sequence in transactions\n",
    "            ])\n",
    "            # Apply FP-Growth algorithm\n",
    "            frequent_itemsets = fpgrowth(transaction_df, min_support=min_support, use_colnames=True, max_len=4)\n",
    "\n",
    "            # Sort by support and keep top results\n",
    "            if not frequent_itemsets.empty:\n",
    "                most_frequent = frequent_itemsets.sort_values(by='support', ascending=False)\n",
    "                most_frequent['itemsets'] = frequent_itemsets['itemsets'].apply(lambda x: list(x))\n",
    "                results[incident] = most_frequent\n",
    "            else:\n",
    "                results[incident] = None\n",
    "            \n",
    "            # store the results in a csv file\n",
    "            most_frequent.to_csv(f'results\\\\results\\\\results_{incident}.csv', sep=';', index=False)\n",
    "        # Run for all the database\n",
    "        transactions = data['events_sequence'].tolist()\n",
    "        unique_events = set(event for sequence in transactions for event in sequence)  # All unique events\n",
    "        transaction_df = pd.DataFrame([\n",
    "            {event: (event in sequence) for event in unique_events} for sequence in transactions\n",
    "        ])\n",
    "        database_frequent_itemsets = fpgrowth(transaction_df, min_support=min_support, use_colnames=True)\n",
    "        database_frequent_itemsets = database_frequent_itemsets.sort_values(by='support', ascending=False)\n",
    "        database_frequent_itemsets['itemsets'] = database_frequent_itemsets['itemsets'].apply(lambda x: list(x))\n",
    "        database_frequent_itemsets.to_csv(f'results\\\\results\\\\results_database.csv', sep=';', index=False)\n",
    "        return results\n",
    "\n",
    "    # Load the dataset\n",
    "    data = pd.read_csv('sncb_prepared.csv', sep=';')\n",
    "\n",
    "    # Run the function\n",
    "    results = find_frequent_itemsets_fp_growth(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################# Final Frequent itemsets #############################\n",
    "if USE_ITEMSETS:\n",
    "    import os\n",
    "    import pandas as pd\n",
    "\n",
    "    itemsets_to_not_add = pd.read_csv('results\\\\results\\\\results_database.csv', sep=';')\n",
    "\n",
    "    frequent_itmesets = []\n",
    "    # read all the files in the results folder\n",
    "    for filename in os.listdir('results\\\\results'):\n",
    "        if filename == 'results_database.csv' or filename == 'results_frequent.csv':\n",
    "            continue\n",
    "        incident_frequent_itemset = pd.read_csv(f'results\\\\results\\\\{filename}', sep=';')\n",
    "        for index, row in incident_frequent_itemset.iterrows():\n",
    "            if row['itemsets'] not in frequent_itmesets and row['itemsets'] not in itemsets_to_not_add['itemsets']:\n",
    "                frequent_itmesets.append(row['itemsets'])\n",
    "\n",
    "    frequent_itmesets = pd.DataFrame(frequent_itmesets, columns=['itemsets'])\n",
    "    frequent_itmesets.to_csv('results\\\\results\\\\results_frequent.csv', sep=';', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################# One hot encoding unique events #############################\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('sncb_prepared.csv', sep=';')\n",
    "data['events_sequence'] = data['events_sequence'].apply(lambda x: list(map(int, x.strip('[]').split(','))))\n",
    "# print(data['duration_sequence'][0])\n",
    "# data['duration_sequence'] = data['duration_sequence'].apply(lambda x: list(map(int, x.strip('[]').split(','))))\n",
    "data['duration_sequence'] = data['duration_sequence'].apply(\n",
    "    lambda x: list(map(int, x.strip('[]').split(','))) if x.strip('[]').strip() else []\n",
    ")\n",
    "\n",
    "\n",
    "# Extract unique events\n",
    "unique_events = sorted({event for seq in data['events_sequence'] for event in seq})\n",
    "event_to_index = {event: idx for idx, event in enumerate(unique_events)}\n",
    "\n",
    "# Create a one-hot encoded matrix\n",
    "ohe_matrix = np.zeros((len(data), len(unique_events)), dtype=int)\n",
    "for i, seq in enumerate(data['events_sequence']):\n",
    "    for event_i, event in enumerate(seq):\n",
    "        ohe_matrix[i, event_to_index[event]] = 1\n",
    "        if not USE_ITEMSETS and len(data['duration_sequence'][i]) > event_i:\n",
    "            ohe_matrix[i, event_to_index[event]] += (data['duration_sequence'][i][event_i] + 1)\n",
    "\n",
    "# Convert to DataFrame\n",
    "ohe_df = pd.DataFrame(ohe_matrix, columns=[str(event) for event in unique_events])\n",
    "\n",
    "# Add the target column\n",
    "ohe_df['target'] = data['incident_type']\n",
    "\n",
    "# Save the OHE data to CSV\n",
    "name = 'unique_events_OHE.csv'\n",
    "if not USE_ITEMSETS:\n",
    "    name = 'duration_OHE.csv'\n",
    "ohe_df.to_csv(name, sep=';', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################# One hot encoding ############################# \n",
    "if USE_ITEMSETS:\n",
    "    import pandas as pd\n",
    "\n",
    "    # Load data\n",
    "    sets = pd.read_csv('results\\\\results\\\\results_frequent.csv', sep=';')\n",
    "    sets = sets['itemsets'].apply(lambda x: list(map(int, x.strip('[]').split(',')))).tolist()\n",
    "\n",
    "    OHE_data = pd.read_csv('unique_events_OHE.csv', sep=';')\n",
    "    unique_events = OHE_data.columns.tolist()\n",
    "    unique_events.remove('target')\n",
    "\n",
    "    # Add new columns for each itemset\n",
    "    new_columns = {str(itemset): 0 for itemset in sets}\n",
    "    new_data = pd.DataFrame(new_columns, index=OHE_data.index)\n",
    "\n",
    "    # Perform one-hot encoding for itemsets\n",
    "    for index, row in OHE_data.iterrows():\n",
    "        for itemset in sets:\n",
    "            if all(row[str(event)] == 1 for event in itemset):\n",
    "                new_data.at[index, str(itemset)] = 1\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    OHE_data.drop(columns=unique_events, inplace=True)\n",
    "\n",
    "    # Combine original and new data\n",
    "    OHE_data = pd.concat([OHE_data, new_data], axis=1)\n",
    "\n",
    "    # Save to file\n",
    "    OHE_data.to_csv('OHE_frequent.csv', sep=';', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ############################# One hot encoding events SPEED #############################\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# # Load the dataset\n",
    "# data = pd.read_csv('sncb_speed.csv', sep=';')\n",
    "\n",
    "# itemsets = []\n",
    "# for filename in os.listdir('relevance\\\\relevance\\\\event_speed_alim'):\n",
    "#     print(\"processing file: \", filename)\n",
    "#     incident_frequent_itemset = pd.read_csv(f'relevance\\\\relevance\\\\event_speed_alim\\\\{filename}', sep=',', nrows=1000)\n",
    "#     for index, row in incident_frequent_itemset.iterrows():\n",
    "#         if row['Sequence'] not in itemsets:\n",
    "#             itemsets.append(row['Sequence'])\n",
    "\n",
    "# print(\"itemsets\")\n",
    "\n",
    "# final_data = pd.DataFrame(0, index=range(len(data)), columns=[str(itemset) for itemset in itemsets])\n",
    "\n",
    "# for index, row in data.iterrows():\n",
    "#     for itemset in itemsets:\n",
    "#         if set(itemset).issubset(set(row['events + speed + alimentation'])):\n",
    "#             final_data.at[index, itemset] = 1\n",
    "\n",
    "# final_data['target'] = data['incident_type']\n",
    "\n",
    "# final_data.to_csv('OHE_speed.csv', sep=';', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import os\n",
    "# from multiprocessing import Pool, cpu_count\n",
    "\n",
    "# # Charger le dataset\n",
    "# data = pd.read_csv('sncb_speed.csv', sep=';')\n",
    "\n",
    "# # Charger les itemsets\n",
    "# def load_itemsets(directory):\n",
    "#     itemsets = set()\n",
    "#     for filename in os.listdir(directory):\n",
    "#         incident_frequent_itemset = pd.read_csv(f'{directory}/{filename}', sep=',')\n",
    "#         for _, row in incident_frequent_itemset.iterrows():\n",
    "#             itemsets.add(tuple(row['Sequence']))\n",
    "#     return list(itemsets)\n",
    "\n",
    "# itemsets = load_itemsets('relevance\\\\relevance\\\\event_speed_alim')\n",
    "\n",
    "# # Initialiser la DataFrame finale avec des colonnes pour chaque itemset\n",
    "# final_data = pd.DataFrame(0, index=range(len(data)), columns=[str(itemset) for itemset in itemsets])\n",
    "\n",
    "# # Fonction pour traiter une ligne de données\n",
    "# def process_row(row):\n",
    "#     row_result = [0] * len(itemsets)\n",
    "#     row_set = set(row['events + speed + alimentation'])\n",
    "#     for i, itemset in enumerate(itemsets):\n",
    "#         if set(itemset).issubset(row_set):\n",
    "#             row_result[i] = 1\n",
    "#     return row_result\n",
    "\n",
    "# # Appliquer parallélisation\n",
    "# def parallel_processing(data):\n",
    "#     with Pool(cpu_count()) as pool:\n",
    "#         results = pool.map(process_row, data.to_dict('records'))\n",
    "#     return results\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     # Convertir les colonnes des résultats parallèles dans la DataFrame finale\n",
    "#     final_data.iloc[:, :] = parallel_processing(data)\n",
    "\n",
    "#     # Ajouter la colonne cible\n",
    "#     final_data['target'] = data['incident_type']\n",
    "\n",
    "#     # Sauvegarder le fichier\n",
    "#     final_data.to_csv('OHE_speed.csv', sep=';', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Model 1: Frequent itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "data = pd.read_csv('OHE_frequent.csv', sep=';')\n",
    "target = data['target']\n",
    "data.drop(columns=['target'], inplace=True)\n",
    "\n",
    "lasso = Lasso(alpha=0.1)\n",
    "lasso.fit(data, target)\n",
    "\n",
    "selector = SelectFromModel(lasso, prefit=True)\n",
    "data_LASSO = selector.transform(data)\n",
    "data_LASSO = pd.DataFrame(data_LASSO)\n",
    "data_LASSO['target'] = target\n",
    "data_LASSO.to_csv('models\\\\train_data\\\\model_1_lasso.csv', sep=';', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('models\\\\train_data\\\\model_1_lasso.csv', sep=';')\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "# Forward feature selection\n",
    "sfs = SequentialFeatureSelector(model,\n",
    "                               n_features_to_select=100,\n",
    "                               direction='forward',\n",
    "                               cv=4)\n",
    "\n",
    "# Fit the model\n",
    "sfs.fit(data.drop('target', axis=1), data['target'])\n",
    "\n",
    "# Get the transformed data with\n",
    "# the selected features\n",
    "data_RFE = sfs.transform(data.drop('target', axis=1))\n",
    "\n",
    "data_RFE = pd.DataFrame(data_RFE)\n",
    "\n",
    "data_RFE['target'] = data['target']\n",
    "data_RFE.to_csv('models\\\\train_data\\\\model_1.csv', sep=';', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Model 2: Event Duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "data = pd.read_csv('duration_OHE.csv', sep=';')\n",
    "target = data['target']\n",
    "data = data.drop('target', axis=1)\n",
    "\n",
    "k = int(len(data.columns) // (5/3))\n",
    "\n",
    "data_ANOVA = SelectKBest(f_classif, k=k).fit_transform(data, target) # find good K\n",
    "data_ANOVA = pd.DataFrame(data_ANOVA)\n",
    "data_ANOVA['target'] = target\n",
    "data_ANOVA.to_csv('models\\\\train_data\\\\model_2.csv', sep=';', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_TESTED = 1 # 1 or 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Importing Libraries and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, matthews_corrcoef\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "CV = 5\n",
    "PATH = f'models\\\\train_data\\\\model_{MODEL_TESTED}.csv'\n",
    "SEP = ';'\n",
    "models_names = ['BNB', 'RFC', 'XGB', 'KNN']\n",
    "reports_file_path = 'models\\\\reports.txt'\n",
    "\n",
    "data = pd.read_csv(PATH, sep=SEP)\n",
    "target = data['target']\n",
    "LABELS = target.unique().tolist()\n",
    "LABELS.sort()\n",
    "\n",
    "def plot_confusion_matrix(cm, model_name, labels):\n",
    "    cm = cm.astype('int') \n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", xticklabels=labels, yticklabels=labels)\n",
    "    plt.savefig(f'models\\\\figures\\\\confusion_matrix_{model_name}.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Creating and storing the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into features and target\n",
    "X = data.drop(columns=['target'])\n",
    "y = data['target']\n",
    "\n",
    "for model_name in models_names:\n",
    "    for cv in range(CV):\n",
    "        # Split the data into training and testing sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\n",
    "        \n",
    "        # SMOTE\n",
    "        # sm = SMOTE(k_neighbors=2)\n",
    "        # X_train, y_train = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "        # Create a classifier\n",
    "        if model_name == 'BNB':\n",
    "            clf = BernoulliNB()\n",
    "        elif model_name == 'RFC':\n",
    "            clf = RandomForestClassifier()\n",
    "        elif model_name == 'XGB':\n",
    "            clf = XGBClassifier()\n",
    "            xgb_lables = np.unique(y_train)\n",
    "            xgb_lables.sort()\n",
    "            lable_dict = {xgb_lables[i]: i for i in range(len(xgb_lables))}\n",
    "            y_train = y_train.map(lable_dict)\n",
    "            y_test = y_test.map(lable_dict)\n",
    "            X_test.columns = X_test.columns.astype(str)\n",
    "            X_test.columns = X_test.columns.str.replace(r\"[^\\w]\", \"_\", regex=True)\n",
    "            X_train.columns = X_train.columns.astype(str)\n",
    "            X_train.columns = X_train.columns.str.replace(r\"[^\\w]\", \"_\", regex=True)\n",
    "        elif model_name == 'KNN':\n",
    "            clf = KNeighborsClassifier()\n",
    "\n",
    "        # Train the classifier\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        # Save the model\n",
    "        joblib.dump(clf, f'models\\\\models\\\\{model_name}_{cv}.pkl')\n",
    "\n",
    "        # Save the test data\n",
    "        X_test.to_csv(f'models\\\\test_data\\\\{model_name}_test_data_{cv}.csv', index=False)\n",
    "        y_test.to_csv(f'models\\\\test_data\\\\{model_name}_test_target_{cv}.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(reports_file_path, 'w') as f:\n",
    "    for model_name in models_names:\n",
    "        mcc = 0\n",
    "        reports_agg = None\n",
    "        for cv in range(CV):\n",
    "            clf = joblib.load(f'models\\\\models\\\\{model_name}_{cv}.pkl')\n",
    "            X_test = pd.read_csv(f'models\\\\test_data\\\\{model_name}_test_data_{cv}.csv')\n",
    "            y_test = pd.read_csv(f'models\\\\test_data\\\\{model_name}_test_target_{cv}.csv')\n",
    "\n",
    "            y_pred = clf.predict(X_test)\n",
    "            cm_cv = confusion_matrix(y_test, y_pred, labels=LABELS)\n",
    "            mcc_cv = matthews_corrcoef(y_test, y_pred)\n",
    "            report_cv = classification_report(y_test, y_pred, output_dict=True, target_names=LABELS, zero_division=0, labels=LABELS)\n",
    "            # reports.append(classification_report(y_test, y_pred, target_names=LABELS, output_dict=True))\n",
    "            if reports_agg is None:\n",
    "                reports_agg = report_cv\n",
    "            else:\n",
    "                for label, metrics in report_cv.items():\n",
    "                    if isinstance(metrics, dict):\n",
    "                        for metric_name, value in metrics.items():\n",
    "                            reports_agg[label][metric_name] += value\n",
    "\n",
    "            mcc += mcc_cv\n",
    "            cm = cm_cv if cv == 0 else cm + cm_cv\n",
    "\n",
    "        # Normalize classification report metrics over CV folds\n",
    "        for label, metrics in reports_agg.items():\n",
    "            if label == \"accuracy\":\n",
    "                continue\n",
    "            if isinstance(metrics, dict):\n",
    "                for metric_name in metrics:\n",
    "                    reports_agg[label][metric_name] /= CV\n",
    "\n",
    "        f.write(f\"Model: {model_name}\\n\")\n",
    "        f.write(f\"Average Matthews Correlation Coefficient: {mcc / CV:.4f}\\n\")\n",
    "        f.write(\"Confusion Matrix:\\n\")\n",
    "        f.write(f\"{cm}\\n\")\n",
    "        f.write(\"Classification Report:\\n\")\n",
    "        f.write(pd.DataFrame(reports_agg).transpose().to_string())\n",
    "        f.write(\"\\n\\n\")\n",
    "\n",
    "        plot_confusion_matrix(cm, model_name, LABELS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
