{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################# Data preparation script for the SNCB Data Challenge #############################\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "MIN_BEFORE_INCIDENT = 15\n",
    "MIN_AFTER_INCIDENT = 5\n",
    "   \n",
    "# Read CSV file\n",
    "data = pd.read_csv('sncb_data_challenge.csv', sep=';')\n",
    "\n",
    "#Convert string to list of integers\n",
    "col_list_int = ['vehicles_sequence', 'events_sequence','seconds_to_incident_sequence']\n",
    "for col in col_list_int:\n",
    "    data[col] = data[col].apply(lambda x: list(map(int, x.strip('[]').split(','))))\n",
    "\n",
    "col_list_float = ['train_kph_sequence']\n",
    "for col in col_list_float:\n",
    "    data[col] = data[col].apply(lambda x: list(map(float, x.strip('[]').split(','))))\n",
    "\n",
    "col_list_str = ['dj_ac_state_sequence', 'dj_dc_state_sequence']\n",
    "for col in col_list_str:\n",
    "    data[col] = data[col].apply(lambda x: list(map(str, x.strip('[]').split(','))))\n",
    "\n",
    "#Convert string to list of floats\n",
    "# data['train_kph_sequence'] = data['train_kph_sequence'].apply(lambda x: list(map(float, x.strip('[]').split(','))))\n",
    "\n",
    "# Compute the acceleration\n",
    "data['acceleration_seq'] = data.apply(\n",
    "    lambda row: [\n",
    "        (row['train_kph_sequence'][i + 1] - row['train_kph_sequence'][i]) / \n",
    "        (row['seconds_to_incident_sequence'][i + 1] - row['seconds_to_incident_sequence'][i])\n",
    "        if (row['seconds_to_incident_sequence'][i + 1] - row['seconds_to_incident_sequence'][i]) != 0 and row['vehicles_sequence'][i+1] == row['vehicles_sequence'][i] else np.nan\n",
    "        for i in range(len(row['train_kph_sequence']) - 1)\n",
    "    ], axis=1)\n",
    "\n",
    "for i in range(len(data['events_sequence'])):\n",
    "    new_vehicles_sequence = []\n",
    "    new_events_sequence = []\n",
    "    new_seconds_to_incident_sequence = []\n",
    "    new_train_kph_sequence = []\n",
    "    new_dj_ac_state_sequence = []\n",
    "    new_dj_dc_state_sequence = []\n",
    "    new_acceleration_seq = []\n",
    "    \n",
    "    start_index = 0\n",
    "    for j in range(len(data['events_sequence'][i])):\n",
    "        is_before_incident = data['seconds_to_incident_sequence'][i][j] >= -MIN_BEFORE_INCIDENT * 60\n",
    "        is_after_incident = data['seconds_to_incident_sequence'][i][j] <= MIN_AFTER_INCIDENT * 60\n",
    "        time_condition = is_before_incident and is_after_incident\n",
    "        if not time_condition:\n",
    "            start_index += 1\n",
    "            continue\n",
    "\n",
    "        \n",
    "        if j == start_index or data['events_sequence'][i][j] != new_events_sequence[-1]:\n",
    "            new_vehicles_sequence.append(data['vehicles_sequence'][i][j])\n",
    "            new_events_sequence.append(data['events_sequence'][i][j])\n",
    "            new_seconds_to_incident_sequence.append(data['seconds_to_incident_sequence'][i][j])\n",
    "            new_train_kph_sequence.append(data['train_kph_sequence'][i][j])\n",
    "            new_dj_ac_state_sequence.append(data['dj_ac_state_sequence'][i][j])\n",
    "            new_dj_dc_state_sequence.append(data['dj_dc_state_sequence'][i][j])\n",
    "            if j < len(data['acceleration_seq'][i]):\n",
    "                new_acceleration_seq.append(data['acceleration_seq'][i][j])\n",
    "    \n",
    "    data.at[i, 'vehicles_sequence'] = new_vehicles_sequence\n",
    "    data.at[i, 'events_sequence'] = new_events_sequence\n",
    "    data.at[i, 'seconds_to_incident_sequence'] = new_seconds_to_incident_sequence\n",
    "    data.at[i, 'train_kph_sequence'] = new_train_kph_sequence\n",
    "    data.at[i, 'dj_ac_state_sequence'] = new_dj_ac_state_sequence\n",
    "    data.at[i, 'dj_dc_state_sequence'] = new_dj_dc_state_sequence\n",
    "    data.at[i, 'acceleration_seq'] = new_acceleration_seq\n",
    "\n",
    "for i in range(len(data['events_sequence'])):\n",
    "    for j in range(len(data['events_sequence'][i]) - 1):\n",
    "        if data['events_sequence'][i][j] == data['events_sequence'][i][j+1]:\n",
    "            print(\"duplicates\")\n",
    "            print(i)\n",
    "            print(len(data['events_sequence'][i]))\n",
    "            print(j)\n",
    "            raise ValueError(\"Duplicates in events_sequence\")\n",
    "    \n",
    "# Save the modified DataFrame to a new CSV file\n",
    "data.to_csv('sncb_prepared.csv', sep=';', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 34\u001b[0m\n\u001b[0;32m     30\u001b[0m     frequent_itemsets \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m support\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m min_support}\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m frequent_itemsets\n\u001b[1;32m---> 34\u001b[0m \u001b[38;5;28mprint\u001b[39m(fb_growth(data, min_support\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m))\n",
      "Cell \u001b[1;32mIn[3], line 23\u001b[0m, in \u001b[0;36mfb_growth\u001b[1;34m(data, min_support)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mevents_sequence\u001b[39m\u001b[38;5;124m'\u001b[39m])):\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mevents_sequence\u001b[39m\u001b[38;5;124m'\u001b[39m])):\n\u001b[1;32m---> 23\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mevents_sequence\u001b[39m\u001b[38;5;124m'\u001b[39m][i] \u001b[38;5;129;01min\u001b[39;00m frequent_items \u001b[38;5;129;01mand\u001b[39;00m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mevents_sequence\u001b[39m\u001b[38;5;124m'\u001b[39m][j] \u001b[38;5;129;01min\u001b[39;00m frequent_items:\n\u001b[0;32m     24\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m (row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mevents_sequence\u001b[39m\u001b[38;5;124m'\u001b[39m][i], row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mevents_sequence\u001b[39m\u001b[38;5;124m'\u001b[39m][j]) \u001b[38;5;129;01min\u001b[39;00m support:\n\u001b[0;32m     25\u001b[0m                 support[(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mevents_sequence\u001b[39m\u001b[38;5;124m'\u001b[39m][i], row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mevents_sequence\u001b[39m\u001b[38;5;124m'\u001b[39m][j])] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\bryan\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:1007\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1004\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[0;32m   1006\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[1;32m-> 1007\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_value(key)\n\u001b[0;32m   1009\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_hashable(key):\n\u001b[0;32m   1010\u001b[0m     \u001b[38;5;66;03m# Otherwise index.get_value will raise InvalidIndexError\u001b[39;00m\n\u001b[0;32m   1011\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1012\u001b[0m         \u001b[38;5;66;03m# For labels that don't resolve as scalars like tuples and frozensets\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\bryan\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:1099\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m   1096\u001b[0m     new_mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mgetitem_mgr(indexer)\n\u001b[0;32m   1097\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor(new_mgr)\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m-> 1099\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_value\u001b[39m(\u001b[38;5;28mself\u001b[39m, label, takeable: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1101\u001b[0m \u001b[38;5;124;03m    Quickly retrieve single value at passed index label.\u001b[39;00m\n\u001b[0;32m   1102\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1110\u001b[0m \u001b[38;5;124;03m    scalar value\u001b[39;00m\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m takeable:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "############################# Fp-Growth (OLD) #############################\n",
    "#Find the most frequent sequence of events for each type of incident in the dataset using the FP-Growth algorithm\n",
    "data = pd.read_csv('sncb_prepared.csv', sep=';')\n",
    "\n",
    "def fb_growth(data, min_support=0.1):\n",
    "    # Create a dictionary to store the support of each item\n",
    "    support = {}\n",
    "    for index, row in data.iterrows():\n",
    "        for event in row['events_sequence']:\n",
    "            if event in support:\n",
    "                support[event] += 1\n",
    "            else:\n",
    "                support[event] = 1\n",
    "\n",
    "    # Filter the items that have a support greater than the minimum support\n",
    "    frequent_items = {k: v for k, v in support.items() if v / len(data) >= min_support}\n",
    "\n",
    "    # Create a dictionary to store the support of each itemset\n",
    "    support = {}\n",
    "    for index, row in data.iterrows():\n",
    "        for i in range(len(row['events_sequence'])):\n",
    "            for j in range(i + 1, len(row['events_sequence'])):\n",
    "                if row['events_sequence'][i] in frequent_items and row['events_sequence'][j] in frequent_items:\n",
    "                    if (row['events_sequence'][i], row['events_sequence'][j]) in support:\n",
    "                        support[(row['events_sequence'][i], row['events_sequence'][j])] += 1\n",
    "                    else:\n",
    "                        support[(row['events_sequence'][i], row['events_sequence'][j])] = 1\n",
    "\n",
    "    # Filter the itemsets that have a support greater than the minimum support\n",
    "    frequent_itemsets = {k: v for k, v in support.items() if v / len(data) >= min_support}\n",
    "\n",
    "    return frequent_itemsets\n",
    "\n",
    "print(fb_growth(data, min_support=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'most_frequent' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 69\u001b[0m\n\u001b[0;32m     66\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msncb_prepared.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# Run the function\u001b[39;00m\n\u001b[1;32m---> 69\u001b[0m results \u001b[38;5;241m=\u001b[39m find_frequent_itemsets_fp_growth(data)\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# Display the results\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m incident, frequent \u001b[38;5;129;01min\u001b[39;00m results\u001b[38;5;241m.\u001b[39mitems():\n",
      "Cell \u001b[1;32mIn[4], line 52\u001b[0m, in \u001b[0;36mfind_frequent_itemsets_fp_growth\u001b[1;34m(data, min_support)\u001b[0m\n\u001b[0;32m     49\u001b[0m         results[incident] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;66;03m# store the results in a csv file\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m     most_frequent\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mresults_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mincident\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# Run for all the database\u001b[39;00m\n\u001b[0;32m     54\u001b[0m transactions \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mevents_sequence\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: cannot access local variable 'most_frequent' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "############################# Frequent itemsets (FP-Growth) #############################\n",
    "\n",
    "import pandas as pd\n",
    "from mlxtend.frequent_patterns import fpgrowth\n",
    "import ast  # For safely evaluating string representations of lists\n",
    "\n",
    "\n",
    "def find_frequent_itemsets_fp_growth(data, min_support=0.9):\n",
    "    \"\"\"\n",
    "    Finds the most frequent sequences of events for each incident type using FP-Growth.\n",
    "    \"\"\"\n",
    "    # Get all unique incident types\n",
    "    incident_types = data['incident_type'].unique()\n",
    "    results = {}\n",
    "\n",
    "    # Convert stringified lists to actual lists of integers\n",
    "    data['events_sequence'] = data['events_sequence'].apply(lambda x: ast.literal_eval(x))\n",
    "\n",
    "    for incident in incident_types:\n",
    "        # Check if the csv file already exists\n",
    "        try:\n",
    "            most_frequent = pd.read_csv(f'results\\\\results_{incident}.csv', sep=';')\n",
    "            if most_frequent is not None:\n",
    "                results[incident] = most_frequent\n",
    "                continue\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # Filter rows for the current incident type\n",
    "        filtered_data = data[data['incident_type'] == incident]\n",
    "\n",
    "        # Prepare transactions: each transaction is a sequence of events\n",
    "        transactions = filtered_data['events_sequence'].tolist()\n",
    "\n",
    "        # Create a one-hot encoded DataFrame for the events\n",
    "        unique_events = set(event for sequence in transactions for event in sequence)  # All unique events\n",
    "        transaction_df = pd.DataFrame([\n",
    "            {event: (event in sequence) for event in unique_events} for sequence in transactions\n",
    "        ])\n",
    "        # Apply FP-Growth algorithm\n",
    "        frequent_itemsets = fpgrowth(transaction_df, min_support=min_support, use_colnames=True)\n",
    "\n",
    "        # Sort by support and keep top results\n",
    "        if not frequent_itemsets.empty:\n",
    "            most_frequent = frequent_itemsets.sort_values(by='support', ascending=False)\n",
    "            most_frequent['itemsets'] = frequent_itemsets['itemsets'].apply(lambda x: list(x))\n",
    "            results[incident] = most_frequent\n",
    "        else:\n",
    "            results[incident] = None\n",
    "        \n",
    "        # store the results in a csv file\n",
    "        most_frequent.to_csv(f'results\\\\results_{incident}.csv', sep=';', index=False)\n",
    "    # Run for all the database\n",
    "    transactions = data['events_sequence'].tolist()\n",
    "    unique_events = set(event for sequence in transactions for event in sequence)  # All unique events\n",
    "    transaction_df = pd.DataFrame([\n",
    "        {event: (event in sequence) for event in unique_events} for sequence in transactions\n",
    "    ])\n",
    "    database_frequent_itemsets = fpgrowth(transaction_df, min_support=min_support, use_colnames=True)\n",
    "    database_frequent_itemsets = database_frequent_itemsets.sort_values(by='support', ascending=False)\n",
    "    database_frequent_itemsets['itemsets'] = database_frequent_itemsets['itemsets'].apply(lambda x: list(x))\n",
    "    database_frequent_itemsets.to_csv(f'results\\\\results_database.csv', sep=';', index=False)\n",
    "    return results\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('sncb_prepared.csv', sep=';')\n",
    "\n",
    "# Run the function\n",
    "results = find_frequent_itemsets_fp_growth(data)\n",
    "\n",
    "# Display the results\n",
    "for incident, frequent in results.items():\n",
    "    print(f\"Incident Type: {incident}\")\n",
    "    if frequent is not None:\n",
    "        print(frequent)\n",
    "    else:\n",
    "        print(\"No frequent sequences found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################# Frequent sequences #############################\n",
    "import pandas as pd\n",
    "\n",
    "def find_frequent_sequences_fp_growth(data, threshold=0.9):\n",
    "    \"\"\"\n",
    "    Finds the most frequent sequences of events for each incident type using GSP.\n",
    "    \"\"\"\n",
    "    # Get all unique incident types\n",
    "    incident_types = data['incident_type'].unique()\n",
    "    results = {}\n",
    "\n",
    "    # Convert stringified lists to actual lists of integers\n",
    "    data['events_sequence'] = data['events_sequence'].apply(lambda x: ast.literal_eval(x))\n",
    "\n",
    "    for incident in incident_types:\n",
    "        # Check if the csv file already exists\n",
    "        try:\n",
    "            most_frequent = pd.read_csv(f'results\\\\results_{incident}.csv', sep=';')\n",
    "            results[incident] = most_frequent\n",
    "            continue\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # Filter rows for the current incident type\n",
    "        filtered_data = data[data['incident_type'] == incident]\n",
    "\n",
    "        # Prepare transactions: each transaction is a sequence of events\n",
    "        transactions = filtered_data['events_sequence'].tolist()\n",
    "\n",
    "        # Create a one-hot encoded DataFrame for the events\n",
    "        unique_events = set(event for sequence in transactions for event in sequence)  # All unique events\n",
    "        transaction_df = pd.DataFrame([\n",
    "            {event: (event in sequence) for event in unique_events} for sequence in transactions\n",
    "        ])\n",
    "        \n",
    "        count_list = np.zeros(len(unique_events))\n",
    "        for i in range(len(unique_events)):\n",
    "            count_list[i] = transaction_df[unique_events[i]].sum()\n",
    "\n",
    "        for i in range(len(unique_events)):\n",
    "            if count_list[i] < threshold*len(transactions):\n",
    "                # transaction_df.drop(columns=[unique_events[i]], inplace=True)\n",
    "                unique_events.remove(unique_events[i])\n",
    "\n",
    "        frequent_sequences = unique_events\n",
    "        new_frq_seq = unique_events\n",
    "        old_frq_seq = unique_events\n",
    "        max_time_diff = 50 # TODO: Define a proper value\n",
    "        while len(new_frq_seq) > 0:\n",
    "            new_frq_seq = []\n",
    "            time_diff = max_time_diff\n",
    "            \n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'results\\\\results_database.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m itemsets_to_not_add \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mresults_database.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m frequent_itmesets \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# read all the files in the results folder\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\bryan\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    900\u001b[0m     dialect,\n\u001b[0;32m    901\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    909\u001b[0m )\n\u001b[0;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\bryan\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\bryan\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32mc:\\Users\\bryan\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1660\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1662\u001b[0m     f,\n\u001b[0;32m   1663\u001b[0m     mode,\n\u001b[0;32m   1664\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1665\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1666\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1667\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1668\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1669\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1670\u001b[0m )\n\u001b[0;32m   1671\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1672\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\bryan\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    854\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    855\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    856\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    857\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    858\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    860\u001b[0m             handle,\n\u001b[0;32m    861\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    862\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    863\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    864\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    865\u001b[0m         )\n\u001b[0;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'results\\\\results_database.csv'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "itemsets_to_not_add = pd.read_csv('results\\\\results_database.csv', sep=';')\n",
    "\n",
    "frequent_itmesets = []\n",
    "# read all the files in the results folder\n",
    "for filename in os.listdir('results'):\n",
    "    if filename == 'results_database.csv':\n",
    "        continue\n",
    "    incident_frequent_itemset = pd.read_csv(f'results\\\\{filename}', sep=';')\n",
    "    for index, row in incident_frequent_itemset.iterrows():\n",
    "        if row['itemsets'] not in frequent_itmesets and row['itemsets'] not in itemsets_to_not_add['itemsets']:\n",
    "            frequent_itmesets.append(row['itemsets'])\n",
    "\n",
    "frequent_itmesets = pd.DataFrame(frequent_itmesets, columns=['itemsets'])\n",
    "frequent_itmesets.to_csv('results\\\\results_frequent.csv', sep=';', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['[2708]', '[4066]', '[4068]', '[4394]', '[2956]', '[3658]', '[3636]',\n",
      "       '[3658, 3636]', '[4168]', '[4140]', '[3658, 2956]', '[2956, 3636]',\n",
      "       '[2956, 3658, 3636]', '[2742]', '[4148]', '[4026]', '[4026, 2708]',\n",
      "       '[4066, 2708]', '[3658, 2708]', '[3658, 2708, 3636]', '[2708, 4068]',\n",
      "       '[2708, 3636]', '[3658, 4066]', '[2956, 2708]', '[4016]',\n",
      "       '[3658, 2708, 4066]', '[4016, 2708]', '[4066, 3636]',\n",
      "       '[3658, 3636, 4066]', '[4066, 2956]', '[3658, 4068]', '[4066, 4068]',\n",
      "       '[4066, 2708, 3636]', '[2708, 4066, 4068]', '[3658, 2708, 3636, 4066]',\n",
      "       '[3658, 4068, 3636]', '[4068, 3636]', '[2956, 4066, 2708]',\n",
      "       '[4016, 4026]', '[4124]', '[2956, 4124]', '[4066, 4124]',\n",
      "       '[4066, 2956, 4124]', '[4068, 3658, 2956]', '[4068, 2956, 3636]',\n",
      "       '[3658, 2956, 3636]', '[4068, 2956]', '[4068, 3658, 2956, 3636]',\n",
      "       'incident_type'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('sncb_prepared.csv', sep=';')\n",
    "frequent_itmesets = pd.read_csv('results\\\\results_frequent.csv', sep=';')\n",
    "columns = frequent_itmesets['itemsets'].tolist()\n",
    "columns.append('incident_type')\n",
    "final_data = pd.DataFrame(columns=columns)\n",
    "\n",
    "print(final_data.columns)\n",
    "\n",
    "# Apply the one hot encoding\n",
    "for index, row in data.iterrows():\n",
    "    \n",
    "    one_hot_encoding = []\n",
    "    for itemset in frequent_itmesets['itemsets']:\n",
    "        itemset = list(itemset.strip('[]').split(','))\n",
    "        events_sequence = list(row['events_sequence'].strip('[]').split(','))\n",
    "        if all(event in events_sequence for event in itemset):\n",
    "            one_hot_encoding.append(1)\n",
    "        else:\n",
    "            one_hot_encoding.append(0)\n",
    "    one_hot_encoding.append(row['incident_type'])\n",
    "    final_data = pd.concat([final_data, pd.DataFrame([one_hot_encoding], columns=columns)], ignore_index=True)\n",
    "\n",
    "final_data.to_csv('sncb_final.csv', sep=';', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144\n",
      "311\n",
      "23\n",
      "1025\n",
      "68\n",
      "412\n",
      "24\n",
      "54\n",
      "99\n",
      "9\n",
      "299\n",
      "14\n",
      "1275\n",
      "311\n",
      "327\n",
      "2505\n",
      "2\n",
      "144\n",
      "25\n",
      "511\n",
      "19\n",
      "4\n",
      "379\n",
      "584\n",
      "311\n",
      "1251\n",
      "305\n",
      "128\n",
      "128\n",
      "1236\n",
      "50\n",
      "65\n",
      "13\n",
      "13\n",
      "1389\n",
      "9\n",
      "703\n",
      "69\n",
      "89\n",
      "13\n",
      "153\n",
      "99\n",
      "13\n",
      "43\n",
      "103\n",
      "1114\n",
      "130\n",
      "9\n",
      "42\n",
      "1708\n",
      "688\n",
      "2\n",
      "447\n",
      "255\n",
      "13\n",
      "2602\n",
      "13\n",
      "54\n",
      "108\n",
      "13\n",
      "122\n",
      "637\n",
      "1479\n",
      "359\n",
      "546\n",
      "13\n",
      "176\n",
      "2398\n",
      "2514\n",
      "63\n",
      "47\n",
      "9\n",
      "20\n",
      "4\n",
      "447\n",
      "208\n",
      "13\n",
      "68\n",
      "13\n",
      "1191\n",
      "13\n",
      "1613\n",
      "276\n",
      "13\n",
      "24\n",
      "58\n",
      "64\n",
      "9\n",
      "1287\n",
      "13\n",
      "384\n",
      "36\n",
      "22\n",
      "153\n",
      "1240\n",
      "67\n",
      "116\n",
      "397\n",
      "1613\n",
      "201\n",
      "13\n",
      "321\n",
      "1623\n",
      "21\n",
      "22\n",
      "4\n",
      "4\n",
      "22\n",
      "17\n",
      "2398\n",
      "20\n",
      "1095\n",
      "13\n",
      "14\n",
      "154\n",
      "488\n",
      "275\n",
      "99\n",
      "221\n",
      "342\n",
      "165\n",
      "107\n",
      "13\n",
      "11\n",
      "669\n",
      "366\n",
      "15\n",
      "480\n",
      "1293\n",
      "25\n",
      "169\n",
      "1758\n",
      "68\n",
      "167\n",
      "1177\n",
      "13\n",
      "175\n",
      "374\n",
      "99\n",
      "190\n",
      "255\n",
      "200\n",
      "800\n",
      "311\n",
      "58\n",
      "1353\n",
      "406\n",
      "13\n",
      "18\n",
      "260\n",
      "2\n",
      "2\n",
      "58\n",
      "13\n",
      "13\n",
      "14\n",
      "222\n",
      "165\n",
      "117\n",
      "1279\n",
      "14\n",
      "14\n",
      "18\n",
      "189\n",
      "158\n",
      "116\n",
      "13\n",
      "25\n",
      "663\n",
      "223\n",
      "165\n",
      "388\n",
      "326\n",
      "1229\n",
      "56\n",
      "11\n",
      "280\n",
      "200\n",
      "1498\n",
      "13\n",
      "14\n",
      "22\n",
      "99\n",
      "800\n",
      "11\n",
      "18\n",
      "386\n",
      "69\n",
      "140\n",
      "397\n",
      "17\n",
      "312\n",
      "283\n",
      "22\n",
      "1617\n",
      "111\n",
      "391\n",
      "2\n",
      "801\n",
      "173\n",
      "1430\n",
      "1153\n",
      "266\n",
      "118\n",
      "161\n",
      "150\n",
      "14\n",
      "280\n",
      "57\n",
      "170\n",
      "16\n",
      "300\n",
      "297\n",
      "99\n",
      "176\n",
      "16\n",
      "300\n",
      "204\n",
      "334\n",
      "141\n",
      "397\n",
      "153\n",
      "2255\n",
      "239\n",
      "3\n",
      "2505\n",
      "236\n",
      "280\n",
      "177\n",
      "13\n",
      "12\n",
      "388\n",
      "2\n",
      "200\n",
      "16\n",
      "37\n",
      "13\n",
      "99\n",
      "18\n",
      "50\n",
      "99\n",
      "13\n",
      "310\n",
      "200\n",
      "14\n",
      "24\n",
      "17\n",
      "200\n",
      "165\n",
      "733\n",
      "307\n",
      "337\n",
      "15\n",
      "300\n",
      "762\n",
      "2\n",
      "104\n",
      "255\n",
      "759\n",
      "461\n",
      "41\n",
      "58\n",
      "22\n",
      "63\n",
      "14\n",
      "23\n",
      "47\n",
      "4\n",
      "99\n",
      "241\n",
      "22\n",
      "281\n",
      "105\n",
      "6\n",
      "56\n",
      "154\n",
      "502\n",
      "1125\n",
      "231\n",
      "378\n",
      "4\n",
      "13\n",
      "201\n",
      "2213\n",
      "680\n",
      "24\n",
      "13\n",
      "2\n",
      "2301\n",
      "1286\n",
      "99\n",
      "201\n",
      "114\n",
      "42\n",
      "14\n",
      "15\n",
      "63\n",
      "451\n",
      "123\n",
      "395\n",
      "13\n",
      "68\n",
      "47\n",
      "16\n",
      "200\n",
      "58\n",
      "312\n",
      "560\n",
      "13\n",
      "17\n",
      "393\n",
      "207\n",
      "201\n",
      "2\n",
      "276\n",
      "311\n",
      "18\n",
      "111\n",
      "200\n",
      "56\n",
      "14\n",
      "2\n",
      "9\n",
      "67\n",
      "874\n",
      "9\n",
      "2\n",
      "307\n",
      "2262\n",
      "2436\n",
      "103\n",
      "68\n",
      "169\n",
      "126\n",
      "169\n",
      "14\n",
      "153\n",
      "144\n",
      "286\n",
      "204\n",
      "165\n",
      "106\n",
      "641\n",
      "248\n",
      "14\n",
      "58\n",
      "577\n",
      "2503\n",
      "13\n",
      "99\n",
      "2445\n",
      "2516\n",
      "15\n",
      "37\n",
      "108\n",
      "285\n",
      "589\n",
      "11\n",
      "13\n",
      "58\n",
      "705\n",
      "14\n",
      "99\n",
      "68\n",
      "286\n",
      "169\n",
      "94\n",
      "22\n",
      "2398\n",
      "14\n",
      "99\n",
      "1586\n",
      "1522\n",
      "107\n",
      "223\n",
      "196\n",
      "189\n",
      "16\n",
      "99\n",
      "144\n",
      "4\n",
      "28\n",
      "296\n",
      "529\n",
      "1132\n",
      "4\n",
      "41\n",
      "201\n",
      "753\n",
      "48\n",
      "480\n",
      "739\n",
      "196\n",
      "68\n",
      "42\n",
      "36\n",
      "1613\n",
      "14\n",
      "384\n",
      "200\n",
      "1244\n",
      "49\n",
      "790\n",
      "11\n",
      "68\n",
      "11\n",
      "67\n",
      "19\n",
      "227\n",
      "303\n",
      "200\n",
      "44\n",
      "62\n",
      "14\n",
      "13\n",
      "67\n",
      "391\n",
      "16\n",
      "319\n",
      "2\n",
      "67\n",
      "4\n",
      "23\n",
      "14\n",
      "51\n",
      "22\n",
      "1529\n",
      "9\n",
      "108\n",
      "20\n",
      "252\n",
      "1409\n",
      "64\n",
      "67\n",
      "220\n",
      "451\n",
      "402\n",
      "403\n",
      "23\n",
      "13\n",
      "153\n",
      "14\n",
      "311\n",
      "227\n",
      "2215\n",
      "201\n",
      "217\n",
      "1125\n",
      "200\n",
      "356\n",
      "99\n",
      "251\n",
      "69\n",
      "388\n",
      "68\n",
      "1315\n",
      "194\n",
      "355\n",
      "122\n",
      "41\n",
      "16\n",
      "7\n",
      "68\n",
      "155\n",
      "105\n",
      "160\n",
      "41\n",
      "46\n",
      "1250\n",
      "11\n",
      "48\n",
      "570\n",
      "4\n",
      "1404\n",
      "68\n",
      "14\n",
      "326\n",
      "349\n",
      "267\n",
      "16\n",
      "18\n",
      "286\n",
      "40\n",
      "58\n",
      "12\n",
      "9\n",
      "68\n",
      "68\n",
      "101\n",
      "67\n",
      "153\n",
      "47\n",
      "153\n",
      "2\n",
      "2398\n",
      "286\n",
      "294\n",
      "99\n",
      "14\n",
      "286\n",
      "99\n",
      "167\n",
      "1533\n",
      "1127\n",
      "24\n",
      "353\n",
      "179\n",
      "67\n",
      "99\n",
      "13\n",
      "11\n",
      "153\n",
      "65\n",
      "38\n",
      "165\n",
      "404\n",
      "40\n",
      "67\n",
      "169\n",
      "385\n",
      "9\n",
      "179\n",
      "155\n",
      "48\n",
      "3\n",
      "99\n",
      "25\n",
      "366\n",
      "200\n",
      "196\n",
      "188\n",
      "141\n",
      "2\n",
      "1251\n",
      "68\n",
      "1286\n",
      "200\n",
      "358\n",
      "67\n",
      "73\n",
      "2\n",
      "2\n",
      "13\n",
      "20\n",
      "153\n",
      "9\n",
      "13\n",
      "742\n",
      "148\n",
      "615\n",
      "11\n",
      "264\n",
      "80\n",
      "13\n",
      "13\n",
      "1081\n",
      "107\n",
      "150\n",
      "13\n",
      "169\n",
      "398\n",
      "17\n",
      "13\n",
      "22\n",
      "391\n",
      "310\n",
      "64\n",
      "43\n",
      "15\n",
      "4\n",
      "226\n",
      "14\n",
      "52\n",
      "622\n",
      "35\n",
      "789\n",
      "99\n",
      "3205\n",
      "275\n",
      "653\n",
      "153\n",
      "739\n",
      "280\n",
      "1494\n",
      "848\n",
      "47\n",
      "283\n",
      "17\n",
      "286\n",
      "815\n",
      "179\n",
      "13\n",
      "125\n",
      "181\n",
      "99\n",
      "13\n",
      "11\n",
      "1052\n",
      "266\n",
      "139\n",
      "36\n",
      "275\n",
      "13\n",
      "173\n",
      "171\n",
      "397\n",
      "16\n",
      "255\n",
      "67\n",
      "1091\n",
      "11\n",
      "13\n",
      "1411\n",
      "516\n",
      "311\n",
      "108\n",
      "22\n",
      "484\n",
      "99\n",
      "800\n",
      "49\n",
      "99\n",
      "99\n",
      "25\n",
      "574\n",
      "153\n",
      "57\n",
      "99\n",
      "22\n",
      "153\n",
      "99\n",
      "541\n",
      "287\n",
      "304\n",
      "173\n",
      "391\n",
      "13\n",
      "376\n",
      "265\n",
      "214\n",
      "186\n",
      "14\n",
      "140\n",
      "1219\n",
      "309\n",
      "309\n",
      "1531\n",
      "17\n",
      "348\n",
      "13\n",
      "577\n",
      "372\n",
      "195\n",
      "14\n",
      "143\n",
      "13\n",
      "99\n",
      "215\n",
      "22\n",
      "69\n",
      "99\n",
      "56\n",
      "2\n",
      "1220\n",
      "24\n",
      "99\n",
      "41\n",
      "238\n",
      "328\n",
      "175\n",
      "99\n",
      "56\n",
      "169\n",
      "743\n",
      "129\n",
      "2392\n",
      "11\n",
      "3134\n",
      "791\n",
      "2602\n",
      "9\n",
      "153\n",
      "58\n",
      "13\n",
      "25\n",
      "46\n",
      "230\n",
      "20\n",
      "9\n",
      "26\n",
      "17\n",
      "259\n",
      "13\n",
      "67\n",
      "13\n",
      "157\n",
      "110\n",
      "187\n",
      "12\n",
      "124\n",
      "99\n",
      "99\n",
      "153\n",
      "99\n",
      "18\n",
      "16\n",
      "393\n",
      "402\n",
      "166\n",
      "2\n",
      "402\n",
      "99\n",
      "14\n",
      "120\n",
      "14\n",
      "99\n",
      "189\n",
      "2482\n",
      "452\n",
      "239\n",
      "108\n",
      "449\n",
      "1620\n",
      "35\n",
      "23\n",
      "41\n",
      "2\n",
      "315\n",
      "11\n",
      "15\n",
      "274\n",
      "169\n",
      "23\n",
      "108\n",
      "129\n",
      "108\n",
      "18\n",
      "796\n",
      "153\n",
      "99\n",
      "2\n",
      "169\n",
      "101\n",
      "2\n",
      "108\n",
      "286\n",
      "13\n",
      "1082\n",
      "14\n",
      "242\n",
      "44\n",
      "68\n",
      "56\n",
      "386\n",
      "1499\n",
      "58\n",
      "23\n",
      "443\n",
      "217\n",
      "14\n",
      "1319\n",
      "94\n",
      "18\n",
      "23\n",
      "886\n",
      "23\n",
      "354\n",
      "59\n",
      "223\n",
      "99\n",
      "99\n",
      "155\n",
      "99\n",
      "43\n",
      "99\n",
      "99\n",
      "20\n",
      "402\n",
      "801\n",
      "341\n",
      "2224\n",
      "109\n",
      "300\n",
      "23\n",
      "153\n",
      "9\n",
      "1654\n",
      "99\n",
      "43\n",
      "156\n",
      "99\n",
      "125\n",
      "99\n",
      "99\n",
      "22\n",
      "13\n",
      "398\n",
      "170\n",
      "712\n",
      "392\n",
      "50\n",
      "647\n",
      "58\n",
      "310\n",
      "280\n",
      "311\n",
      "2\n",
      "114\n",
      "22\n",
      "78\n",
      "48\n",
      "68\n",
      "189\n",
      "58\n",
      "4\n",
      "1531\n",
      "1229\n",
      "622\n",
      "108\n",
      "67\n",
      "113\n",
      "118\n",
      "354\n",
      "480\n",
      "176\n",
      "60\n",
      "161\n",
      "13\n",
      "13\n",
      "109\n",
      "109\n",
      "312\n",
      "67\n",
      "275\n",
      "6\n",
      "9\n",
      "4\n",
      "88\n",
      "765\n",
      "19\n",
      "76\n",
      "2\n",
      "300\n",
      "228\n",
      "280\n",
      "19\n",
      "22\n",
      "22\n",
      "47\n",
      "13\n",
      "99\n",
      "102\n",
      "22\n",
      "67\n",
      "114\n",
      "11\n",
      "13\n",
      "705\n",
      "21\n",
      "44\n",
      "4\n",
      "220\n",
      "148\n",
      "800\n",
      "284\n",
      "276\n",
      "311\n",
      "183\n",
      "129\n",
      "227\n",
      "165\n",
      "44\n",
      "416\n",
      "13\n",
      "14\n",
      "365\n",
      "213\n",
      "219\n",
      "103\n",
      "22\n",
      "741\n",
      "275\n",
      "163\n",
      "13\n",
      "169\n",
      "169\n",
      "99\n",
      "108\n",
      "99\n",
      "56\n",
      "255\n",
      "120\n",
      "14\n",
      "17\n",
      "420\n",
      "326\n",
      "1499\n",
      "209\n",
      "108\n",
      "43\n",
      "135\n",
      "104\n",
      "23\n",
      "9\n",
      "191\n",
      "127\n",
      "99\n",
      "58\n",
      "189\n",
      "16\n",
      "483\n",
      "886\n",
      "56\n",
      "294\n",
      "16\n",
      "67\n",
      "4\n",
      "125\n",
      "40\n",
      "63\n",
      "17\n",
      "100\n",
      "69\n",
      "154\n",
      "109\n",
      "20\n",
      "210\n",
      "170\n",
      "169\n",
      "49\n",
      "14\n",
      "789\n",
      "16\n",
      "385\n",
      "99\n",
      "14\n",
      "11\n",
      "11\n",
      "13\n",
      "108\n",
      "13\n",
      "13\n",
      "81\n",
      "118\n",
      "741\n",
      "154\n",
      "13\n",
      "275\n",
      "365\n",
      "1533\n",
      "13\n",
      "169\n",
      "22\n",
      "99\n",
      "14\n",
      "1219\n",
      "169\n",
      "341\n",
      "19\n",
      "23\n",
      "80\n",
      "488\n",
      "9\n",
      "22\n",
      "14\n",
      "80\n",
      "4\n",
      "20\n",
      "255\n",
      "217\n",
      "959\n",
      "14\n",
      "199\n",
      "269\n",
      "248\n",
      "133\n",
      "46\n",
      "23\n",
      "4\n",
      "2\n",
      "58\n",
      "133\n",
      "81\n",
      "35\n",
      "427\n",
      "70\n",
      "656\n",
      "196\n",
      "18\n",
      "200\n",
      "427\n",
      "758\n",
      "37\n",
      "13\n",
      "4\n",
      "218\n",
      "113\n",
      "1203\n",
      "441\n",
      "99\n",
      "68\n",
      "2\n",
      "209\n",
      "391\n",
      "9\n",
      "2\n",
      "68\n",
      "2\n",
      "1522\n",
      "169\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "for index, row in final_data.iterrows():\n",
    "    print(sum(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the rows of the DataFrame\n",
    "data = pd.read_csv('sncb_prepared.csv', sep=';')\n",
    "print(len(data['events_sequence']))\n",
    "counter = 0\n",
    "for index, row in data.iterrows():\n",
    "    # check if the numbers 2708, 4026, 4068, 4066 appear is in the events_sequence\n",
    "    if '2708' in row['events_sequence'] and '4026' in row['events_sequence'] and '4068' in row['events_sequence'] and '4066' in row['events_sequence']:\n",
    "        counter += 1\n",
    "\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1]\n"
     ]
    }
   ],
   "source": [
    "#change '[1 , 1]' in real list [1, 1]\n",
    "string = '[1 , 1]'\n",
    "print(list(map(int, string.strip('[]').split(','))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
