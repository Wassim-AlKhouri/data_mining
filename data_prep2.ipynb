{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################# Data preparation script for the SNCB Data Challenge #############################\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "MIN_BEFORE_INCIDENT = 120\n",
    "MIN_AFTER_INCIDENT = 60\n",
    "   \n",
    "# Read CSV file\n",
    "data = pd.read_csv('sncb_data_challenge.csv', sep=';')\n",
    "\n",
    "# Convert string to list of integers\n",
    "col_list_int = ['vehicles_sequence', 'events_sequence','seconds_to_incident_sequence']\n",
    "for col in col_list_int:\n",
    "    data[col] = data[col].apply(lambda x: list(map(int, x.strip('[]').split(','))))\n",
    "\n",
    "# Convert string to list of floats\n",
    "col_list_float = ['train_kph_sequence']\n",
    "for col in col_list_float:\n",
    "    data[col] = data[col].apply(lambda x: list(map(float, x.strip('[]').split(','))))\n",
    "\n",
    "# Convert string to list\n",
    "col_list_str = ['dj_ac_state_sequence', 'dj_dc_state_sequence']\n",
    "for col in col_list_str:\n",
    "    data[col] = data[col].apply(lambda x: list(map(str, x.strip('[]').split(','))))\n",
    "\n",
    "# Create a new column for the duration of each event\n",
    "data['duration_sequence'] = [[] for _ in range(len(data))]\n",
    "\n",
    "# Compute the acceleration\n",
    "data['acceleration_seq'] = data.apply(\n",
    "    lambda row: [\n",
    "        (row['train_kph_sequence'][i + 1] - row['train_kph_sequence'][i]) / \n",
    "        (row['seconds_to_incident_sequence'][i + 1] - row['seconds_to_incident_sequence'][i])\n",
    "        if (row['seconds_to_incident_sequence'][i + 1] - row['seconds_to_incident_sequence'][i]) != 0 and row['vehicles_sequence'][i+1] == row['vehicles_sequence'][i] else np.nan\n",
    "        for i in range(len(row['train_kph_sequence']) - 1)\n",
    "    ], axis=1)\n",
    "\n",
    "# Remove redundant events\n",
    "for i in range(len(data['events_sequence'])):\n",
    "    new_vehicles_sequence = []\n",
    "    new_events_sequence = []\n",
    "    new_seconds_to_incident_sequence = []\n",
    "    new_train_kph_sequence = []\n",
    "    new_dj_ac_state_sequence = []\n",
    "    new_dj_dc_state_sequence = []\n",
    "    new_acceleration_seq = []\n",
    "    duration_sequence = []\n",
    "    \n",
    "    start_index = 0\n",
    "    # duration = 0\n",
    "    for j in range(len(data['events_sequence'][i]) - 1):\n",
    "        is_before_incident = data['seconds_to_incident_sequence'][i][j] >= -MIN_BEFORE_INCIDENT * 60\n",
    "        is_after_incident = data['seconds_to_incident_sequence'][i][j] <= MIN_AFTER_INCIDENT * 60\n",
    "        time_condition = is_before_incident and is_after_incident\n",
    "        # Skip row that are not in the time window\n",
    "        if not time_condition:\n",
    "            start_index = j+1\n",
    "            continue\n",
    "        \n",
    "        # Keep every event that is not the same as the previous one\n",
    "        if j == start_index or data['events_sequence'][i][j] != new_events_sequence[-1]:\n",
    "            new_vehicles_sequence.append(data['vehicles_sequence'][i][j])\n",
    "            new_events_sequence.append(data['events_sequence'][i][j])\n",
    "            new_seconds_to_incident_sequence.append(data['seconds_to_incident_sequence'][i][j])\n",
    "            new_train_kph_sequence.append(data['train_kph_sequence'][i][j])\n",
    "            new_dj_ac_state_sequence.append(data['dj_ac_state_sequence'][i][j])\n",
    "            new_dj_dc_state_sequence.append(data['dj_dc_state_sequence'][i][j])\n",
    "            if j < len(data['acceleration_seq'][i]):\n",
    "                new_acceleration_seq.append(data['acceleration_seq'][i][j])\n",
    "            duration_sequence.append(0)\n",
    "\n",
    "        # Compute the duration of the event\n",
    "        elif data['events_sequence'][i][j] == new_events_sequence[-1]:\n",
    "            duration_sequence[-1] += abs(data['seconds_to_incident_sequence'][i][j] - data['seconds_to_incident_sequence'][i][j-1])\n",
    "            \n",
    "    if len(new_events_sequence) == 0:\n",
    "        continue\n",
    "    \n",
    "    data.at[i, 'vehicles_sequence'] = new_vehicles_sequence\n",
    "    data.at[i, 'events_sequence'] = new_events_sequence\n",
    "    data.at[i, 'seconds_to_incident_sequence'] = new_seconds_to_incident_sequence\n",
    "    data.at[i, 'train_kph_sequence'] = new_train_kph_sequence\n",
    "    data.at[i, 'dj_ac_state_sequence'] = new_dj_ac_state_sequence\n",
    "    data.at[i, 'dj_dc_state_sequence'] = new_dj_dc_state_sequence\n",
    "    data.at[i, 'acceleration_seq'] = new_acceleration_seq\n",
    "    data.at[i, 'duration_sequence'] = duration_sequence\n",
    "\n",
    "# for i in range(len(data['events_sequence'])):\n",
    "#     if len(data['events_sequence'][i]) == 0:\n",
    "#         print(\"empty\")\n",
    "    \n",
    "# Save the modified DataFrame to a new CSV file\n",
    "data.to_csv('sncb_prepared.csv', sep=';', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################# Frequent itemsets (FP-Growth) #############################\n",
    "\n",
    "import pandas as pd\n",
    "from mlxtend.frequent_patterns import fpgrowth\n",
    "import ast  # For safely evaluating string representations of lists\n",
    "\n",
    "MIN_SUPPORT = 0.8\n",
    "\n",
    "def find_frequent_itemsets_fp_growth(data, min_support=MIN_SUPPORT):\n",
    "    \"\"\"\n",
    "    Finds the most frequent sequences of events for each incident type using FP-Growth.\n",
    "    \"\"\"\n",
    "    # Get all unique incident types\n",
    "    incident_types = data['incident_type'].unique()\n",
    "    results = {}\n",
    "\n",
    "    # Convert stringified lists to actual lists of integers\n",
    "    data['events_sequence'] = data['events_sequence'].apply(lambda x: ast.literal_eval(x))\n",
    "\n",
    "    for incident in incident_types:\n",
    "        # Check if the csv file already exists\n",
    "        # try:\n",
    "        #     most_frequent = pd.read_csv(f'results\\\\results_{incident}.csv', sep=';')\n",
    "        #     if most_frequent is not None:\n",
    "        #         results[incident] = most_frequent\n",
    "        #         continue\n",
    "        # except:\n",
    "        #     pass\n",
    "\n",
    "        # Filter rows for the current incident type\n",
    "        filtered_data = data[data['incident_type'] == incident]\n",
    "\n",
    "        # Prepare transactions: each transaction is a sequence of events\n",
    "        transactions = filtered_data['events_sequence'].tolist()\n",
    "\n",
    "        # Create a one-hot encoded DataFrame for the events\n",
    "        unique_events = set(event for sequence in transactions for event in sequence)  # All unique events\n",
    "        transaction_df = pd.DataFrame([\n",
    "            {event: (event in sequence) for event in unique_events} for sequence in transactions\n",
    "        ])\n",
    "        # Apply FP-Growth algorithm\n",
    "        frequent_itemsets = fpgrowth(transaction_df, min_support=min_support, use_colnames=True)\n",
    "\n",
    "        # Sort by support and keep top results\n",
    "        if not frequent_itemsets.empty:\n",
    "            most_frequent = frequent_itemsets.sort_values(by='support', ascending=False)\n",
    "            most_frequent['itemsets'] = frequent_itemsets['itemsets'].apply(lambda x: list(x))\n",
    "            results[incident] = most_frequent\n",
    "        else:\n",
    "            results[incident] = None\n",
    "        \n",
    "        # store the results in a csv file\n",
    "        most_frequent.to_csv(f'results\\\\results_{incident}.csv', sep=';', index=False)\n",
    "    # Run for all the database\n",
    "    transactions = data['events_sequence'].tolist()\n",
    "    unique_events = set(event for sequence in transactions for event in sequence)  # All unique events\n",
    "    transaction_df = pd.DataFrame([\n",
    "        {event: (event in sequence) for event in unique_events} for sequence in transactions\n",
    "    ])\n",
    "    database_frequent_itemsets = fpgrowth(transaction_df, min_support=min_support, use_colnames=True)\n",
    "    database_frequent_itemsets = database_frequent_itemsets.sort_values(by='support', ascending=False)\n",
    "    database_frequent_itemsets['itemsets'] = database_frequent_itemsets['itemsets'].apply(lambda x: list(x))\n",
    "    database_frequent_itemsets.to_csv(f'results\\\\results_database.csv', sep=';', index=False)\n",
    "    return results\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('sncb_prepared.csv', sep=';')\n",
    "\n",
    "# Run the function\n",
    "results = find_frequent_itemsets_fp_growth(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################# Frequent sequences #############################\n",
    "# import pandas as pd\n",
    "\n",
    "# def find_frequent_sequences_fp_growth(data, threshold=0.9):\n",
    "#     \"\"\"\n",
    "#     Finds the most frequent sequences of events for each incident type using GSP.\n",
    "#     \"\"\"\n",
    "#     # Get all unique incident types\n",
    "#     incident_types = data['incident_type'].unique()\n",
    "#     results = {}\n",
    "\n",
    "#     # Convert stringified lists to actual lists of integers\n",
    "#     data['events_sequence'] = data['events_sequence'].apply(lambda x: ast.literal_eval(x))\n",
    "\n",
    "#     for incident in incident_types:\n",
    "#         # Check if the csv file already exists\n",
    "#         try:\n",
    "#             most_frequent = pd.read_csv(f'results\\\\results_{incident}.csv', sep=';')\n",
    "#             results[incident] = most_frequent\n",
    "#             continue\n",
    "#         except:\n",
    "#             pass\n",
    "\n",
    "#         # Filter rows for the current incident type\n",
    "#         filtered_data = data[data['incident_type'] == incident]\n",
    "\n",
    "#         # Prepare transactions: each transaction is a sequence of events\n",
    "#         transactions = filtered_data['events_sequence'].tolist()\n",
    "\n",
    "#         # Create a one-hot encoded DataFrame for the events\n",
    "#         unique_events = set(event for sequence in transactions for event in sequence)  # All unique events\n",
    "#         transaction_df = pd.DataFrame([\n",
    "#             {event: (event in sequence) for event in unique_events} for sequence in transactions\n",
    "#         ])\n",
    "        \n",
    "#         count_list = np.zeros(len(unique_events))\n",
    "#         for i in range(len(unique_events)):\n",
    "#             count_list[i] = transaction_df[unique_events[i]].sum()\n",
    "\n",
    "#         for i in range(len(unique_events)):\n",
    "#             if count_list[i] < threshold*len(transactions):\n",
    "#                 # transaction_df.drop(columns=[unique_events[i]], inplace=True)\n",
    "#                 unique_events.remove(unique_events[i])\n",
    "\n",
    "#         frequent_sequences = unique_events\n",
    "#         new_frq_seq = unique_events\n",
    "#         old_frq_seq = unique_events\n",
    "#         max_time_diff = 50 # TODO: Define a proper value\n",
    "#         while len(new_frq_seq) > 0:\n",
    "#             new_frq_seq = []\n",
    "#             time_diff = max_time_diff\n",
    "            \n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################# Final Frequent itemsets #############################\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "itemsets_to_not_add = pd.read_csv('results\\\\results_database.csv', sep=';')\n",
    "\n",
    "frequent_itmesets = []\n",
    "# read all the files in the results folder\n",
    "for filename in os.listdir('results'):\n",
    "    if filename == 'results_database.csv' or filename == 'results_frequent.csv':\n",
    "        continue\n",
    "    incident_frequent_itemset = pd.read_csv(f'results\\\\{filename}', sep=';')\n",
    "    for index, row in incident_frequent_itemset.iterrows():\n",
    "        if row['itemsets'] not in frequent_itmesets and row['itemsets'] not in itemsets_to_not_add['itemsets']:\n",
    "            frequent_itmesets.append(row['itemsets'])\n",
    "\n",
    "frequent_itmesets = pd.DataFrame(frequent_itmesets, columns=['itemsets'])\n",
    "frequent_itmesets.to_csv('results\\\\results_frequent.csv', sep=';', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################# One hot encoding ############################# \n",
    "# import pandas as pd\n",
    "\n",
    "# # Load the dataset\n",
    "# data = pd.read_csv('sncb_prepared.csv', sep=';')\n",
    "# frequent_itmesets = pd.read_csv('results\\\\results_frequent.csv', sep=';')\n",
    "# columns = frequent_itmesets['itemsets'].tolist()\n",
    "# columns.append('target')\n",
    "# final_data = pd.DataFrame(columns=columns)\n",
    "\n",
    "# # Apply the one hot encoding\n",
    "# for index, row in data.iterrows():\n",
    "    \n",
    "#     one_hot_encoding = []\n",
    "#     for itemset in frequent_itmesets['itemsets']:\n",
    "#         itemset = list(itemset.strip('[]').split(','))\n",
    "#         events_sequence = list(row['events_sequence'].strip('[]').split(','))\n",
    "#         if all(event in events_sequence for event in itemset):\n",
    "#             one_hot_encoding.append(1)\n",
    "#         else:\n",
    "#             one_hot_encoding.append(0)\n",
    "#     one_hot_encoding.append(row['incident_type'])\n",
    "#     final_data = pd.concat([final_data, pd.DataFrame([one_hot_encoding], columns=columns)], ignore_index=True)\n",
    "\n",
    "# final_data.to_csv('sncb_final.csv', sep=';', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################# One hot encoding unique events #############################\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('sncb_prepared.csv', sep=';')\n",
    "data['events_sequence'] = data['events_sequence'].apply(lambda x: list(map(int, x.strip('[]').split(','))))\n",
    "\n",
    "# Extract unique events\n",
    "unique_events = sorted({event for seq in data['events_sequence'] for event in seq})\n",
    "event_to_index = {event: idx for idx, event in enumerate(unique_events)}\n",
    "\n",
    "# Create a one-hot encoded matrix\n",
    "ohe_matrix = np.zeros((len(data), len(unique_events)), dtype=int)\n",
    "for i, seq in enumerate(data['events_sequence']):\n",
    "    for event in seq:\n",
    "        ohe_matrix[i, event_to_index[event]] = 1\n",
    "\n",
    "# Convert to DataFrame\n",
    "ohe_df = pd.DataFrame(ohe_matrix, columns=[str(event) for event in unique_events])\n",
    "\n",
    "# Add the target column\n",
    "ohe_df['target'] = data['incident_type']\n",
    "\n",
    "# Save the OHE data to CSV\n",
    "ohe_df.to_csv('unique_events_OHE.csv', sep=';', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################# One hot encoding ############################# \n",
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "sets = pd.read_csv('results\\\\results_frequent.csv', sep=';')\n",
    "sets = sets['itemsets'].apply(lambda x: list(map(int, x.strip('[]').split(',')))).tolist()\n",
    "\n",
    "OHE_data = pd.read_csv('unique_events_OHE.csv', sep=';')\n",
    "unique_events = OHE_data.columns.tolist()\n",
    "unique_events.remove('target')\n",
    "\n",
    "# Add new columns for each itemset\n",
    "new_columns = {str(itemset): 0 for itemset in sets}\n",
    "new_data = pd.DataFrame(new_columns, index=OHE_data.index)\n",
    "\n",
    "# Perform one-hot encoding for itemsets\n",
    "for index, row in OHE_data.iterrows():\n",
    "    for itemset in sets:\n",
    "        if all(row[str(event)] == 1 for event in itemset):\n",
    "            new_data.at[index, str(itemset)] = 1\n",
    "\n",
    "# Drop unnecessary columns\n",
    "OHE_data.drop(columns=unique_events, inplace=True)\n",
    "\n",
    "# Combine original and new data\n",
    "OHE_data = pd.concat([OHE_data, new_data], axis=1)\n",
    "\n",
    "# Save to file\n",
    "OHE_data.to_csv('OHE_frequent.csv', sep=';', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
