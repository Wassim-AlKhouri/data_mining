{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################# Data preparation script for the SNCB Data Challenge #############################\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "MIN_BEFORE_INCIDENT = 120\n",
    "MIN_AFTER_INCIDENT = 60\n",
    "\n",
    "USE_ITEMSETS = False\n",
    "   \n",
    "# Read CSV file\n",
    "data = pd.read_csv('sncb_data_challenge.csv', sep=';')\n",
    "\n",
    "# Convert string to list of integers\n",
    "col_list_int = ['vehicles_sequence', 'events_sequence','seconds_to_incident_sequence']\n",
    "for col in col_list_int:\n",
    "    data[col] = data[col].apply(lambda x: list(map(int, x.strip('[]').split(','))))\n",
    "\n",
    "# Convert string to list of floats\n",
    "col_list_float = ['train_kph_sequence']\n",
    "for col in col_list_float:\n",
    "    data[col] = data[col].apply(lambda x: list(map(float, x.strip('[]').split(','))))\n",
    "\n",
    "# Convert string to list\n",
    "col_list_str = ['dj_ac_state_sequence', 'dj_dc_state_sequence']\n",
    "for col in col_list_str:\n",
    "    data[col] = data[col].apply(lambda x: list(map(str, x.strip('[]').split(','))))\n",
    "\n",
    "# Create a new column for the duration of each event\n",
    "data['duration_sequence'] = [[] for _ in range(len(data))]\n",
    "\n",
    "# Compute the acceleration\n",
    "data['acceleration_seq'] = data.apply(\n",
    "    lambda row: [\n",
    "        (row['train_kph_sequence'][i + 1] - row['train_kph_sequence'][i]) / \n",
    "        (row['seconds_to_incident_sequence'][i + 1] - row['seconds_to_incident_sequence'][i])\n",
    "        if (row['seconds_to_incident_sequence'][i + 1] - row['seconds_to_incident_sequence'][i]) != 0 and row['vehicles_sequence'][i+1] == row['vehicles_sequence'][i] else np.nan\n",
    "        for i in range(len(row['train_kph_sequence']) - 1)\n",
    "    ], axis=1)\n",
    "\n",
    "# Remove redundant events\n",
    "for i in range(len(data['events_sequence'])):\n",
    "    new_vehicles_sequence = []\n",
    "    new_events_sequence = []\n",
    "    new_seconds_to_incident_sequence = []\n",
    "    new_train_kph_sequence = []\n",
    "    new_dj_ac_state_sequence = []\n",
    "    new_dj_dc_state_sequence = []\n",
    "    new_acceleration_seq = []\n",
    "    duration_sequence = []\n",
    "    \n",
    "    start_index = 0\n",
    "    # duration = 0\n",
    "    for j in range(len(data['events_sequence'][i]) - 1):\n",
    "        is_before_incident = data['seconds_to_incident_sequence'][i][j] >= -MIN_BEFORE_INCIDENT * 60\n",
    "        is_after_incident = data['seconds_to_incident_sequence'][i][j] <= MIN_AFTER_INCIDENT * 60\n",
    "        time_condition = is_before_incident and is_after_incident\n",
    "        # Skip row that are not in the time window\n",
    "        if not time_condition:\n",
    "            start_index = j+1\n",
    "            continue\n",
    "        \n",
    "        # Keep every event that is not the same as the previous one\n",
    "        if j == start_index or data['events_sequence'][i][j] != new_events_sequence[-1]:\n",
    "            new_vehicles_sequence.append(data['vehicles_sequence'][i][j])\n",
    "            new_events_sequence.append(data['events_sequence'][i][j])\n",
    "            new_seconds_to_incident_sequence.append(data['seconds_to_incident_sequence'][i][j])\n",
    "            new_train_kph_sequence.append(data['train_kph_sequence'][i][j])\n",
    "            new_dj_ac_state_sequence.append(data['dj_ac_state_sequence'][i][j])\n",
    "            new_dj_dc_state_sequence.append(data['dj_dc_state_sequence'][i][j])\n",
    "            if j < len(data['acceleration_seq'][i]):\n",
    "                new_acceleration_seq.append(data['acceleration_seq'][i][j])\n",
    "            duration_sequence.append(0)\n",
    "\n",
    "        # Compute the duration of the event\n",
    "        elif data['events_sequence'][i][j] == new_events_sequence[-1]:\n",
    "            duration_sequence[-1] += abs(data['seconds_to_incident_sequence'][i][j] - data['seconds_to_incident_sequence'][i][j-1])\n",
    "\n",
    "    \n",
    "        # Add the last duration\n",
    "        duration_sequence[-1] += abs(data['seconds_to_incident_sequence'][i][j+1] - data['seconds_to_incident_sequence'][i][j])        \n",
    "    if len(new_events_sequence) == 0:\n",
    "        continue\n",
    "    \n",
    "    data.at[i, 'vehicles_sequence'] = new_vehicles_sequence\n",
    "    data.at[i, 'events_sequence'] = new_events_sequence\n",
    "    data.at[i, 'seconds_to_incident_sequence'] = new_seconds_to_incident_sequence\n",
    "    data.at[i, 'train_kph_sequence'] = new_train_kph_sequence\n",
    "    data.at[i, 'dj_ac_state_sequence'] = new_dj_ac_state_sequence\n",
    "    data.at[i, 'dj_dc_state_sequence'] = new_dj_dc_state_sequence\n",
    "    data.at[i, 'acceleration_seq'] = new_acceleration_seq\n",
    "    data.at[i, 'duration_sequence'] = duration_sequence\n",
    "\n",
    "# for i in range(len(data['events_sequence'])):\n",
    "#     if len(data['events_sequence'][i]) == 0:\n",
    "#         print(\"empty\")\n",
    "    \n",
    "# Save the modified DataFrame to a new CSV file\n",
    "data.to_csv('sncb_prepared.csv', sep=';', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################# Frequent itemsets (FP-Growth) #############################\n",
    "if USE_ITEMSETS:\n",
    "    import pandas as pd\n",
    "    from mlxtend.frequent_patterns import fpgrowth\n",
    "    import ast  # For safely evaluating string representations of lists\n",
    "\n",
    "    MIN_SUPPORT = 0.7\n",
    "\n",
    "    def find_frequent_itemsets_fp_growth(data, min_support=MIN_SUPPORT):\n",
    "        \"\"\"\n",
    "        Finds the most frequent sequences of events for each incident type using FP-Growth.\n",
    "        \"\"\"\n",
    "        # Get all unique incident types\n",
    "        incident_types = data['incident_type'].unique()\n",
    "        results = {}\n",
    "\n",
    "        # Convert stringified lists to actual lists of integers\n",
    "        data['events_sequence'] = data['events_sequence'].apply(lambda x: ast.literal_eval(x))\n",
    "\n",
    "        for incident in incident_types:\n",
    "            # Check if the csv file already exists\n",
    "            # try:\n",
    "            #     most_frequent = pd.read_csv(f'results\\\\results_{incident}.csv', sep=';')\n",
    "            #     if most_frequent is not None:\n",
    "            #         results[incident] = most_frequent\n",
    "            #         continue\n",
    "            # except:\n",
    "            #     pass\n",
    "\n",
    "            # Filter rows for the current incident type\n",
    "            filtered_data = data[data['incident_type'] == incident]\n",
    "\n",
    "            # Prepare transactions: each transaction is a sequence of events\n",
    "            transactions = filtered_data['events_sequence'].tolist()\n",
    "\n",
    "            # Create a one-hot encoded DataFrame for the events\n",
    "            unique_events = set(event for sequence in transactions for event in sequence)  # All unique events\n",
    "            transaction_df = pd.DataFrame([\n",
    "                {event: (event in sequence) for event in unique_events} for sequence in transactions\n",
    "            ])\n",
    "            print(transaction_df)\n",
    "            # Apply FP-Growth algorithm\n",
    "            frequent_itemsets = fpgrowth(transaction_df, min_support=min_support, use_colnames=True)\n",
    "\n",
    "            # Sort by support and keep top results\n",
    "            if not frequent_itemsets.empty:\n",
    "                most_frequent = frequent_itemsets.sort_values(by='support', ascending=False)\n",
    "                most_frequent['itemsets'] = frequent_itemsets['itemsets'].apply(lambda x: list(x))\n",
    "                results[incident] = most_frequent\n",
    "            else:\n",
    "                results[incident] = None\n",
    "            \n",
    "            # store the results in a csv file\n",
    "            most_frequent.to_csv(f'results\\\\results\\\\results_{incident}.csv', sep=';', index=False)\n",
    "        # Run for all the database\n",
    "        transactions = data['events_sequence'].tolist()\n",
    "        unique_events = set(event for sequence in transactions for event in sequence)  # All unique events\n",
    "        transaction_df = pd.DataFrame([\n",
    "            {event: (event in sequence) for event in unique_events} for sequence in transactions\n",
    "        ])\n",
    "        database_frequent_itemsets = fpgrowth(transaction_df, min_support=min_support, use_colnames=True)\n",
    "        database_frequent_itemsets = database_frequent_itemsets.sort_values(by='support', ascending=False)\n",
    "        database_frequent_itemsets['itemsets'] = database_frequent_itemsets['itemsets'].apply(lambda x: list(x))\n",
    "        database_frequent_itemsets.to_csv(f'results\\\\results\\\\results_database.csv', sep=';', index=False)\n",
    "        return results\n",
    "\n",
    "    # Load the dataset\n",
    "    data = pd.read_csv('sncb_prepared.csv', sep=';')\n",
    "\n",
    "    # Run the function\n",
    "    results = find_frequent_itemsets_fp_growth(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################# Final Frequent itemsets #############################\n",
    "if USE_ITEMSETS:\n",
    "    import os\n",
    "    import pandas as pd\n",
    "\n",
    "    itemsets_to_not_add = pd.read_csv('results\\\\results\\\\results_database.csv', sep=';')\n",
    "\n",
    "    frequent_itmesets = []\n",
    "    # read all the files in the results folder\n",
    "    for filename in os.listdir('results\\\\results'):\n",
    "        if filename == 'results_database.csv' or filename == 'results_frequent.csv':\n",
    "            continue\n",
    "        incident_frequent_itemset = pd.read_csv(f'results\\\\results\\\\{filename}', sep=';')\n",
    "        for index, row in incident_frequent_itemset.iterrows():\n",
    "            if row['itemsets'] not in frequent_itmesets and row['itemsets'] not in itemsets_to_not_add['itemsets']:\n",
    "                frequent_itmesets.append(row['itemsets'])\n",
    "\n",
    "    frequent_itmesets = pd.DataFrame(frequent_itmesets, columns=['itemsets'])\n",
    "    frequent_itmesets.to_csv('results\\\\results\\\\results_frequent.csv', sep=';', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################# One hot encoding unique events #############################\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('sncb_prepared.csv', sep=';')\n",
    "data['events_sequence'] = data['events_sequence'].apply(lambda x: list(map(int, x.strip('[]').split(','))))\n",
    "# print(data['duration_sequence'][0])\n",
    "# data['duration_sequence'] = data['duration_sequence'].apply(lambda x: list(map(int, x.strip('[]').split(','))))\n",
    "data['duration_sequence'] = data['duration_sequence'].apply(\n",
    "    lambda x: list(map(int, x.strip('[]').split(','))) if x.strip('[]').strip() else []\n",
    ")\n",
    "\n",
    "\n",
    "# Extract unique events\n",
    "unique_events = sorted({event for seq in data['events_sequence'] for event in seq})\n",
    "event_to_index = {event: idx for idx, event in enumerate(unique_events)}\n",
    "\n",
    "# Create a one-hot encoded matrix\n",
    "ohe_matrix = np.zeros((len(data), len(unique_events)), dtype=int)\n",
    "for i, seq in enumerate(data['events_sequence']):\n",
    "    for event_i, event in enumerate(seq):\n",
    "        ohe_matrix[i, event_to_index[event]] = 1\n",
    "        if not USE_ITEMSETS and len(data['duration_sequence'][i]) > event_i:\n",
    "            ohe_matrix[i, event_to_index[event]] += (data['duration_sequence'][i][event_i] + 1)\n",
    "\n",
    "# Convert to DataFrame\n",
    "ohe_df = pd.DataFrame(ohe_matrix, columns=[str(event) for event in unique_events])\n",
    "\n",
    "# Add the target column\n",
    "ohe_df['target'] = data['incident_type']\n",
    "\n",
    "# Save the OHE data to CSV\n",
    "name = 'unique_events_OHE.csv'\n",
    "if not USE_ITEMSETS:\n",
    "    name = 'duration_OHE.csv'\n",
    "ohe_df.to_csv(name, sep=';', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################# One hot encoding ############################# \n",
    "if USE_ITEMSETS:\n",
    "    import pandas as pd\n",
    "\n",
    "    # Load data\n",
    "    sets = pd.read_csv('results\\\\results\\\\results_frequent.csv', sep=';')\n",
    "    sets = sets['itemsets'].apply(lambda x: list(map(int, x.strip('[]').split(',')))).tolist()\n",
    "\n",
    "    OHE_data = pd.read_csv('unique_events_OHE.csv', sep=';')\n",
    "    unique_events = OHE_data.columns.tolist()\n",
    "    unique_events.remove('target')\n",
    "\n",
    "    # Add new columns for each itemset\n",
    "    new_columns = {str(itemset): 0 for itemset in sets}\n",
    "    new_data = pd.DataFrame(new_columns, index=OHE_data.index)\n",
    "\n",
    "    # Perform one-hot encoding for itemsets\n",
    "    for index, row in OHE_data.iterrows():\n",
    "        for itemset in sets:\n",
    "            if all(row[str(event)] == 1 for event in itemset):\n",
    "                new_data.at[index, str(itemset)] = 1\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    OHE_data.drop(columns=unique_events, inplace=True)\n",
    "\n",
    "    # Combine original and new data\n",
    "    OHE_data = pd.concat([OHE_data, new_data], axis=1)\n",
    "\n",
    "    # Save to file\n",
    "    OHE_data.to_csv('OHE_frequent.csv', sep=';', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m index, row \u001b[38;5;129;01min\u001b[39;00m incident_frequent_itemset\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[0;32m     13\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSequence\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m itemsets:\n\u001b[1;32m---> 14\u001b[0m             itemsets\u001b[38;5;241m.\u001b[39mappend(\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSequence\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[0;32m     16\u001b[0m final_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame()\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m itemset \u001b[38;5;129;01min\u001b[39;00m itemsets:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\series.py:1096\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1095\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[1;32m-> 1096\u001b[0m     \u001b[43mcheck_dict_or_set_indexers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1097\u001b[0m     key \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m   1099\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mEllipsis\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\indexing.py:2765\u001b[0m, in \u001b[0;36mcheck_dict_or_set_indexers\u001b[1;34m(key)\u001b[0m\n\u001b[0;32m   2753\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2754\u001b[0m \u001b[38;5;124;03m    Returns\u001b[39;00m\n\u001b[0;32m   2755\u001b[0m \u001b[38;5;124;03m    -------\u001b[39;00m\n\u001b[0;32m   2756\u001b[0m \u001b[38;5;124;03m    bool\u001b[39;00m\n\u001b[0;32m   2757\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   2758\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m   2759\u001b[0m         obj\u001b[38;5;241m.\u001b[39mstart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mstop \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m (obj\u001b[38;5;241m.\u001b[39mstep \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mstep \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m   2762\u001b[0m     )\n\u001b[1;32m-> 2765\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_dict_or_set_indexers\u001b[39m(key) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2766\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2767\u001b[0m \u001b[38;5;124;03m    Check if the indexer is or contains a dict or set, which is no longer allowed.\u001b[39;00m\n\u001b[0;32m   2768\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   2769\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2770\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mset\u001b[39m)\n\u001b[0;32m   2771\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mtuple\u001b[39m)\n\u001b[0;32m   2772\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mset\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m key)\n\u001b[0;32m   2773\u001b[0m     ):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "############################# One hot encoding events SPEED #############################\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('sncb_speed.csv', sep=';')\n",
    "# data['events_sequence'] = data['events_sequence'].apply(lambda x: list(map(int, x.strip('[]').split(','))))\n",
    "\n",
    "itemsets = []\n",
    "for filename in os.listdir('relevance\\\\relevance\\\\event_speed_alim'):\n",
    "    incident_frequent_itemset = pd.read_csv(f'relevance\\\\relevance\\\\event_speed_alim\\\\{filename}', sep=',')\n",
    "    for index, row in incident_frequent_itemset.iterrows():\n",
    "        if row['Sequence'] not in itemsets:\n",
    "            itemsets.append(row['Sequence'])\n",
    "\n",
    "final_data = pd.DataFrame()\n",
    "\n",
    "for itemset in itemsets:\n",
    "    final_data[itemset] = 0\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    for itemset in itemsets:\n",
    "        if set(itemset).issubset(set(row)):\n",
    "            final_data.at[index, itemset] = 1\n",
    "\n",
    "final_data['target'] = data['incident_type']\n",
    "\n",
    "final_data.to_csv('OHE_speed.csv', sep=';', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "# Charger le dataset\n",
    "data = pd.read_csv('sncb_speed.csv', sep=';')\n",
    "\n",
    "# Charger les itemsets\n",
    "def load_itemsets(directory):\n",
    "    itemsets = set()\n",
    "    for filename in os.listdir(directory):\n",
    "        incident_frequent_itemset = pd.read_csv(f'{directory}/{filename}', sep=',')\n",
    "        for _, row in incident_frequent_itemset.iterrows():\n",
    "            itemsets.add(tuple(row['Sequence']))\n",
    "    return list(itemsets)\n",
    "\n",
    "itemsets = load_itemsets('relevance\\\\relevance\\\\event_speed_alim')\n",
    "\n",
    "# Initialiser la DataFrame finale avec des colonnes pour chaque itemset\n",
    "final_data = pd.DataFrame(0, index=range(len(data)), columns=[str(itemset) for itemset in itemsets])\n",
    "\n",
    "# Fonction pour traiter une ligne de données\n",
    "def process_row(row):\n",
    "    row_result = [0] * len(itemsets)\n",
    "    row_set = set(row['events + speed + alimentation'])\n",
    "    for i, itemset in enumerate(itemsets):\n",
    "        if set(itemset).issubset(row_set):\n",
    "            row_result[i] = 1\n",
    "    return row_result\n",
    "\n",
    "# Appliquer parallélisation\n",
    "def parallel_processing(data):\n",
    "    with Pool(cpu_count()) as pool:\n",
    "        results = pool.map(process_row, data.to_dict('records'))\n",
    "    return results\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Convertir les colonnes des résultats parallèles dans la DataFrame finale\n",
    "    final_data.iloc[:, :] = parallel_processing(data)\n",
    "\n",
    "    # Ajouter la colonne cible\n",
    "    final_data['target'] = data['incident_type']\n",
    "\n",
    "    # Sauvegarder le fichier\n",
    "    final_data.to_csv('OHE_speed.csv', sep=';', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
