{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################# Data preparation script for the SNCB Data Challenge #############################\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "MIN_BEFORE_INCIDENT = 15\n",
    "MIN_AFTER_INCIDENT = 5\n",
    "   \n",
    "# Read CSV file\n",
    "data = pd.read_csv('sncb_data_challenge.csv', sep=';')\n",
    "\n",
    "#Convert string to list of integers\n",
    "col_list_int = ['vehicles_sequence', 'events_sequence','seconds_to_incident_sequence']\n",
    "for col in col_list_int:\n",
    "    data[col] = data[col].apply(lambda x: list(map(int, x.strip('[]').split(','))))\n",
    "\n",
    "col_list_float = ['train_kph_sequence']\n",
    "for col in col_list_float:\n",
    "    data[col] = data[col].apply(lambda x: list(map(float, x.strip('[]').split(','))))\n",
    "\n",
    "col_list_str = ['dj_ac_state_sequence', 'dj_dc_state_sequence']\n",
    "for col in col_list_str:\n",
    "    data[col] = data[col].apply(lambda x: list(map(str, x.strip('[]').split(','))))\n",
    "\n",
    "#Convert string to list of floats\n",
    "# data['train_kph_sequence'] = data['train_kph_sequence'].apply(lambda x: list(map(float, x.strip('[]').split(','))))\n",
    "\n",
    "# Compute the acceleration\n",
    "data['acceleration_seq'] = data.apply(\n",
    "    lambda row: [\n",
    "        (row['train_kph_sequence'][i + 1] - row['train_kph_sequence'][i]) / \n",
    "        (row['seconds_to_incident_sequence'][i + 1] - row['seconds_to_incident_sequence'][i])\n",
    "        if (row['seconds_to_incident_sequence'][i + 1] - row['seconds_to_incident_sequence'][i]) != 0 and row['vehicles_sequence'][i+1] == row['vehicles_sequence'][i] else np.nan\n",
    "        for i in range(len(row['train_kph_sequence']) - 1)\n",
    "    ], axis=1)\n",
    "\n",
    "for i in range(len(data['events_sequence'])):\n",
    "    new_vehicles_sequence = []\n",
    "    new_events_sequence = []\n",
    "    new_seconds_to_incident_sequence = []\n",
    "    new_train_kph_sequence = []\n",
    "    new_dj_ac_state_sequence = []\n",
    "    new_dj_dc_state_sequence = []\n",
    "    new_acceleration_seq = []\n",
    "    \n",
    "    start_index = 0\n",
    "    for j in range(len(data['events_sequence'][i])):\n",
    "        is_before_incident = data['seconds_to_incident_sequence'][i][j] >= -MIN_BEFORE_INCIDENT * 60\n",
    "        is_after_incident = data['seconds_to_incident_sequence'][i][j] <= MIN_AFTER_INCIDENT * 60\n",
    "        time_condition = is_before_incident and is_after_incident\n",
    "        if not time_condition:\n",
    "            start_index += 1\n",
    "            continue\n",
    "\n",
    "        \n",
    "        if j == start_index or data['events_sequence'][i][j] != new_events_sequence[-1]:\n",
    "            new_vehicles_sequence.append(data['vehicles_sequence'][i][j])\n",
    "            new_events_sequence.append(data['events_sequence'][i][j])\n",
    "            new_seconds_to_incident_sequence.append(data['seconds_to_incident_sequence'][i][j])\n",
    "            new_train_kph_sequence.append(data['train_kph_sequence'][i][j])\n",
    "            new_dj_ac_state_sequence.append(data['dj_ac_state_sequence'][i][j])\n",
    "            new_dj_dc_state_sequence.append(data['dj_dc_state_sequence'][i][j])\n",
    "            if j < len(data['acceleration_seq'][i]):\n",
    "                new_acceleration_seq.append(data['acceleration_seq'][i][j])\n",
    "    \n",
    "    data.at[i, 'vehicles_sequence'] = new_vehicles_sequence\n",
    "    data.at[i, 'events_sequence'] = new_events_sequence\n",
    "    data.at[i, 'seconds_to_incident_sequence'] = new_seconds_to_incident_sequence\n",
    "    data.at[i, 'train_kph_sequence'] = new_train_kph_sequence\n",
    "    data.at[i, 'dj_ac_state_sequence'] = new_dj_ac_state_sequence\n",
    "    data.at[i, 'dj_dc_state_sequence'] = new_dj_dc_state_sequence\n",
    "    data.at[i, 'acceleration_seq'] = new_acceleration_seq\n",
    "\n",
    "for i in range(len(data['events_sequence'])):\n",
    "    for j in range(len(data['events_sequence'][i]) - 1):\n",
    "        if data['events_sequence'][i][j] == data['events_sequence'][i][j+1]:\n",
    "            print(\"duplicates\")\n",
    "            print(i)\n",
    "            print(len(data['events_sequence'][i]))\n",
    "            print(j)\n",
    "            raise ValueError(\"Duplicates in events_sequence\")\n",
    "    \n",
    "# Save the modified DataFrame to a new CSV file\n",
    "data.to_csv('sncb_prepared.csv', sep=';', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################# Fp-Growth (OLD) #############################\n",
    "#Find the most frequent sequence of events for each type of incident in the dataset using the FP-Growth algorithm\n",
    "data = pd.read_csv('sncb_prepared.csv', sep=';')\n",
    "\n",
    "def fb_growth(data, min_support=0.1):\n",
    "    # Create a dictionary to store the support of each item\n",
    "    support = {}\n",
    "    for index, row in data.iterrows():\n",
    "        for event in row['events_sequence']:\n",
    "            if event in support:\n",
    "                support[event] += 1\n",
    "            else:\n",
    "                support[event] = 1\n",
    "\n",
    "    # Filter the items that have a support greater than the minimum support\n",
    "    frequent_items = {k: v for k, v in support.items() if v / len(data) >= min_support}\n",
    "\n",
    "    # Create a dictionary to store the support of each itemset\n",
    "    support = {}\n",
    "    for index, row in data.iterrows():\n",
    "        for i in range(len(row['events_sequence'])):\n",
    "            for j in range(i + 1, len(row['events_sequence'])):\n",
    "                if row['events_sequence'][i] in frequent_items and row['events_sequence'][j] in frequent_items:\n",
    "                    if (row['events_sequence'][i], row['events_sequence'][j]) in support:\n",
    "                        support[(row['events_sequence'][i], row['events_sequence'][j])] += 1\n",
    "                    else:\n",
    "                        support[(row['events_sequence'][i], row['events_sequence'][j])] = 1\n",
    "\n",
    "    # Filter the itemsets that have a support greater than the minimum support\n",
    "    frequent_itemsets = {k: v for k, v in support.items() if v / len(data) >= min_support}\n",
    "\n",
    "    return frequent_itemsets\n",
    "\n",
    "print(fb_growth(data, min_support=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incident Type: 4\n",
      "    support      itemsets\n",
      "0  0.807692        [2708]\n",
      "3  0.641026        [4068]\n",
      "1  0.602564        [3658]\n",
      "2  0.602564        [3636]\n",
      "4  0.602564  [3658, 3636]\n",
      "Incident Type: 13\n",
      "    support      itemsets\n",
      "0  0.679245        [2956]\n",
      "1  0.666667        [3658]\n",
      "2  0.666667        [3636]\n",
      "3  0.666667  [3658, 3636]\n",
      "Incident Type: 14\n",
      "    support      itemsets\n",
      "1  0.677852        [4168]\n",
      "5  0.671141        [2956]\n",
      "0  0.657718        [2708]\n",
      "3  0.651007        [3658]\n",
      "4  0.644295        [3636]\n",
      "6  0.644295  [3658, 3636]\n",
      "2  0.630872        [4140]\n",
      "Incident Type: 2\n",
      "     support                  itemsets\n",
      "0   0.907563                    [2708]\n",
      "6   0.798319                    [4066]\n",
      "1   0.781513                    [3658]\n",
      "27  0.764706              [4066, 2708]\n",
      "2   0.739496                    [3636]\n",
      "11  0.739496              [3658, 3636]\n",
      "3   0.731092                    [4068]\n",
      "8   0.722689              [3658, 2708]\n",
      "7   0.705882                    [2956]\n",
      "4   0.697479                    [4026]\n",
      "14  0.689076        [3658, 2708, 3636]\n",
      "18  0.689076              [2708, 4068]\n",
      "12  0.689076              [2708, 3636]\n",
      "24  0.689076              [4026, 2708]\n",
      "9   0.680672              [3658, 4066]\n",
      "28  0.680672              [2956, 2708]\n",
      "5   0.672269                    [4016]\n",
      "10  0.655462        [3658, 2708, 4066]\n",
      "25  0.655462              [4016, 2708]\n",
      "13  0.647059              [4066, 3636]\n",
      "15  0.647059        [3658, 3636, 4066]\n",
      "29  0.630252              [4066, 2956]\n",
      "19  0.630252              [3658, 4068]\n",
      "21  0.630252              [4066, 4068]\n",
      "16  0.621849        [4066, 2708, 3636]\n",
      "23  0.621849        [2708, 4066, 4068]\n",
      "17  0.621849  [3658, 2708, 3636, 4066]\n",
      "22  0.613445        [3658, 4068, 3636]\n",
      "20  0.613445              [4068, 3636]\n",
      "31  0.613445        [2956, 4066, 2708]\n",
      "26  0.605042              [4016, 4026]\n",
      "30  0.605042              [3658, 2956]\n",
      "Incident Type: 11\n",
      "    support itemsets\n",
      "0  0.769231   [2708]\n",
      "1  0.692308   [4066]\n",
      "2  0.653846   [4068]\n",
      "3  0.615385   [4394]\n",
      "Incident Type: 99\n",
      "No frequent sequences found.\n",
      "Incident Type: 9\n",
      "     support            itemsets\n",
      "0   0.897436              [3658]\n",
      "1   0.897436              [3636]\n",
      "5   0.897436        [3658, 3636]\n",
      "2   0.794872              [4066]\n",
      "6   0.743590        [4066, 3636]\n",
      "7   0.743590        [3658, 4066]\n",
      "8   0.743590  [3658, 3636, 4066]\n",
      "3   0.735043              [4068]\n",
      "4   0.700855              [2956]\n",
      "9   0.675214        [4068, 3636]\n",
      "10  0.675214        [3658, 4068]\n",
      "11  0.675214  [3658, 4068, 3636]\n",
      "12  0.675214        [2956, 3636]\n",
      "13  0.675214        [3658, 2956]\n",
      "15  0.675214  [3658, 2956, 3636]\n",
      "14  0.606838        [4066, 2956]\n",
      "Incident Type: 17\n",
      "   support      itemsets\n",
      "2      0.8        [2708]\n",
      "0      0.6        [2742]\n",
      "1      0.6        [4148]\n",
      "3      0.6        [4026]\n",
      "4      0.6        [2956]\n",
      "5      0.6  [4026, 2708]\n",
      "Incident Type: 3\n",
      "    support            itemsets\n",
      "0       1.0              [4066]\n",
      "3       0.8              [2956]\n",
      "9       0.8        [4066, 2956]\n",
      "1       0.6              [3658]\n",
      "2       0.6              [3636]\n",
      "4       0.6              [4124]\n",
      "5       0.6        [3658, 4066]\n",
      "6       0.6        [3658, 3636]\n",
      "7       0.6        [4066, 3636]\n",
      "8       0.6  [3658, 3636, 4066]\n",
      "10      0.6        [2956, 4124]\n",
      "11      0.6        [4066, 4124]\n",
      "12      0.6  [4066, 2956, 4124]\n",
      "Incident Type: 16\n",
      "   support            itemsets\n",
      "0     0.75              [3658]\n",
      "1     0.75              [3636]\n",
      "2     0.75              [2956]\n",
      "3     0.75        [3658, 3636]\n",
      "4     0.75        [3658, 2956]\n",
      "5     0.75        [2956, 3636]\n",
      "6     0.75  [2956, 3658, 3636]\n",
      "Incident Type: 6\n",
      "No frequent sequences found.\n",
      "Incident Type: 7\n",
      "    support                  itemsets\n",
      "0      1.00                    [4068]\n",
      "9      0.75        [3658, 4068, 3636]\n",
      "15     0.75        [4068, 3658, 2956]\n",
      "14     0.75        [4068, 2956, 3636]\n",
      "13     0.75        [3658, 2956, 3636]\n",
      "12     0.75              [4068, 2956]\n",
      "11     0.75              [3658, 2956]\n",
      "10     0.75              [2956, 3636]\n",
      "8      0.75              [4068, 3636]\n",
      "1      0.75                    [4066]\n",
      "7      0.75              [3658, 3636]\n",
      "6      0.75              [3658, 4068]\n",
      "5      0.75              [4066, 4068]\n",
      "4      0.75                    [2956]\n",
      "3      0.75                    [3636]\n",
      "2      0.75                    [3658]\n",
      "16     0.75  [4068, 3658, 2956, 3636]\n"
     ]
    }
   ],
   "source": [
    "############################# Frequent itemsets (FP-Growth) #############################\n",
    "\n",
    "import pandas as pd\n",
    "from mlxtend.frequent_patterns import fpgrowth\n",
    "import ast  # For safely evaluating string representations of lists\n",
    "\n",
    "\n",
    "def find_frequent_itemsets_fp_growth(data, min_support=0.6):\n",
    "    \"\"\"\n",
    "    Finds the most frequent sequences of events for each incident type using FP-Growth.\n",
    "    \"\"\"\n",
    "    # Get all unique incident types\n",
    "    incident_types = data['incident_type'].unique()\n",
    "    results = {}\n",
    "\n",
    "    # Convert stringified lists to actual lists of integers\n",
    "    data['events_sequence'] = data['events_sequence'].apply(lambda x: ast.literal_eval(x))\n",
    "\n",
    "    for incident in incident_types:\n",
    "        # Check if the csv file already exists\n",
    "        try:\n",
    "            most_frequent = pd.read_csv(f'results\\\\results_{incident}.csv', sep=';')\n",
    "            if most_frequent is not None:\n",
    "                results[incident] = most_frequent\n",
    "                continue\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # Filter rows for the current incident type\n",
    "        filtered_data = data[data['incident_type'] == incident]\n",
    "\n",
    "        # Prepare transactions: each transaction is a sequence of events\n",
    "        transactions = filtered_data['events_sequence'].tolist()\n",
    "\n",
    "        # Create a one-hot encoded DataFrame for the events\n",
    "        unique_events = set(event for sequence in transactions for event in sequence)  # All unique events\n",
    "        transaction_df = pd.DataFrame([\n",
    "            {event: (event in sequence) for event in unique_events} for sequence in transactions\n",
    "        ])\n",
    "        # Apply FP-Growth algorithm\n",
    "        frequent_itemsets = fpgrowth(transaction_df, min_support=min_support, use_colnames=True)\n",
    "\n",
    "        # Sort by support and keep top results\n",
    "        if not frequent_itemsets.empty:\n",
    "            most_frequent = frequent_itemsets.sort_values(by='support', ascending=False)\n",
    "            most_frequent['itemsets'] = frequent_itemsets['itemsets'].apply(lambda x: list(x))\n",
    "            results[incident] = most_frequent\n",
    "        else:\n",
    "            results[incident] = None\n",
    "        \n",
    "        # store the results in a csv file\n",
    "        most_frequent.to_csv(f'results\\\\results_{incident}.csv', sep=';', index=False)\n",
    "    # Run for all the database\n",
    "    transactions = data['events_sequence'].tolist()\n",
    "    unique_events = set(event for sequence in transactions for event in sequence)  # All unique events\n",
    "    transaction_df = pd.DataFrame([\n",
    "        {event: (event in sequence) for event in unique_events} for sequence in transactions\n",
    "    ])\n",
    "    database_frequent_itemsets = fpgrowth(transaction_df, min_support=min_support, use_colnames=True)\n",
    "    database_frequent_itemsets = database_frequent_itemsets.sort_values(by='support', ascending=False)\n",
    "    database_frequent_itemsets['itemsets'] = database_frequent_itemsets['itemsets'].apply(lambda x: list(x))\n",
    "    database_frequent_itemsets.to_csv(f'results\\\\results_database.csv', sep=';', index=False)\n",
    "    return results\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('sncb_prepared.csv', sep=';')\n",
    "\n",
    "# Run the function\n",
    "results = find_frequent_itemsets_fp_growth(data)\n",
    "\n",
    "# Display the results\n",
    "for incident, frequent in results.items():\n",
    "    print(f\"Incident Type: {incident}\")\n",
    "    if frequent is not None:\n",
    "        print(frequent)\n",
    "    else:\n",
    "        print(\"No frequent sequences found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################# Frequent sequences #############################\n",
    "import pandas as pd\n",
    "\n",
    "def find_frequent_sequences_fp_growth(data, threshold=0.9):\n",
    "    \"\"\"\n",
    "    Finds the most frequent sequences of events for each incident type using GSP.\n",
    "    \"\"\"\n",
    "    # Get all unique incident types\n",
    "    incident_types = data['incident_type'].unique()\n",
    "    results = {}\n",
    "\n",
    "    # Convert stringified lists to actual lists of integers\n",
    "    data['events_sequence'] = data['events_sequence'].apply(lambda x: ast.literal_eval(x))\n",
    "\n",
    "    for incident in incident_types:\n",
    "        # Check if the csv file already exists\n",
    "        try:\n",
    "            most_frequent = pd.read_csv(f'results\\\\results_{incident}.csv', sep=';')\n",
    "            results[incident] = most_frequent\n",
    "            continue\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # Filter rows for the current incident type\n",
    "        filtered_data = data[data['incident_type'] == incident]\n",
    "\n",
    "        # Prepare transactions: each transaction is a sequence of events\n",
    "        transactions = filtered_data['events_sequence'].tolist()\n",
    "\n",
    "        # Create a one-hot encoded DataFrame for the events\n",
    "        unique_events = set(event for sequence in transactions for event in sequence)  # All unique events\n",
    "        transaction_df = pd.DataFrame([\n",
    "            {event: (event in sequence) for event in unique_events} for sequence in transactions\n",
    "        ])\n",
    "        \n",
    "        count_list = np.zeros(len(unique_events))\n",
    "        for i in range(len(unique_events)):\n",
    "            count_list[i] = transaction_df[unique_events[i]].sum()\n",
    "\n",
    "        for i in range(len(unique_events)):\n",
    "            if count_list[i] < threshold*len(transactions):\n",
    "                # transaction_df.drop(columns=[unique_events[i]], inplace=True)\n",
    "                unique_events.remove(unique_events[i])\n",
    "\n",
    "        frequent_sequences = unique_events\n",
    "        new_frq_seq = unique_events\n",
    "        old_frq_seq = unique_events\n",
    "        max_time_diff = 50 # TODO: Define a proper value\n",
    "        while len(new_frq_seq) > 0:\n",
    "            new_frq_seq = []\n",
    "            time_diff = max_time_diff\n",
    "            \n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "itemsets_to_not_add = pd.read_csv('results\\\\results_database.csv', sep=';')\n",
    "\n",
    "frequent_itmesets = []\n",
    "# read all the files in the results folder\n",
    "for filename in os.listdir('results'):\n",
    "    if filename == 'results_database.csv':\n",
    "        continue\n",
    "    incident_frequent_itemset = pd.read_csv(f'results\\\\{filename}', sep=';')\n",
    "    for index, row in incident_frequent_itemset.iterrows():\n",
    "        if row['itemsets'] not in frequent_itmesets and row['itemsets'] not in itemsets_to_not_add['itemsets']:\n",
    "            frequent_itmesets.append(row['itemsets'])\n",
    "\n",
    "frequent_itmesets = pd.DataFrame(frequent_itmesets, columns=['itemsets'])\n",
    "frequent_itmesets.to_csv('results\\\\results_frequent.csv', sep=';', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['[2708]', '[4066]', '[4068]', '[4394]', '[2956]', '[3658]', '[3636]',\n",
      "       '[3658, 3636]', '[4168]', '[4140]', '[3658, 2956]', '[2956, 3636]',\n",
      "       '[2956, 3658, 3636]', '[2742]', '[4148]', '[4026]', '[4026, 2708]',\n",
      "       '[4066, 2708]', '[3658, 2708]', '[3658, 2708, 3636]', '[2708, 4068]',\n",
      "       '[2708, 3636]', '[3658, 4066]', '[2956, 2708]', '[4016]',\n",
      "       '[3658, 2708, 4066]', '[4016, 2708]', '[4066, 3636]',\n",
      "       '[3658, 3636, 4066]', '[4066, 2956]', '[3658, 4068]', '[4066, 4068]',\n",
      "       '[4066, 2708, 3636]', '[2708, 4066, 4068]', '[3658, 2708, 3636, 4066]',\n",
      "       '[3658, 4068, 3636]', '[4068, 3636]', '[2956, 4066, 2708]',\n",
      "       '[4016, 4026]', '[4124]', '[2956, 4124]', '[4066, 4124]',\n",
      "       '[4066, 2956, 4124]', '[4068, 3658, 2956]', '[4068, 2956, 3636]',\n",
      "       '[3658, 2956, 3636]', '[4068, 2956]', '[4068, 3658, 2956, 3636]',\n",
      "       'incident_type'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('sncb_prepared.csv', sep=';')\n",
    "frequent_itmesets = pd.read_csv('results\\\\results_frequent.csv', sep=';')\n",
    "columns = frequent_itmesets['itemsets'].tolist()\n",
    "columns.append('incident_type')\n",
    "final_data = pd.DataFrame(columns=columns)\n",
    "\n",
    "print(final_data.columns)\n",
    "\n",
    "# Apply the one hot encoding\n",
    "for index, row in data.iterrows():\n",
    "    \n",
    "    one_hot_encoding = []\n",
    "    for itemset in frequent_itmesets['itemsets']:\n",
    "        itemset = list(itemset.strip('[]').split(','))\n",
    "        events_sequence = list(row['events_sequence'].strip('[]').split(','))\n",
    "        if all(event in events_sequence for event in itemset):\n",
    "            one_hot_encoding.append(1)\n",
    "        else:\n",
    "            one_hot_encoding.append(0)\n",
    "    one_hot_encoding.append(row['incident_type'])\n",
    "    final_data = pd.concat([final_data, pd.DataFrame([one_hot_encoding], columns=columns)], ignore_index=True)\n",
    "\n",
    "final_data.to_csv('sncb_final.csv', sep=';', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144\n",
      "311\n",
      "23\n",
      "1025\n",
      "68\n",
      "412\n",
      "24\n",
      "54\n",
      "99\n",
      "9\n",
      "299\n",
      "14\n",
      "1275\n",
      "311\n",
      "327\n",
      "2505\n",
      "2\n",
      "144\n",
      "25\n",
      "511\n",
      "19\n",
      "4\n",
      "379\n",
      "584\n",
      "311\n",
      "1251\n",
      "305\n",
      "128\n",
      "128\n",
      "1236\n",
      "50\n",
      "65\n",
      "13\n",
      "13\n",
      "1389\n",
      "9\n",
      "703\n",
      "69\n",
      "89\n",
      "13\n",
      "153\n",
      "99\n",
      "13\n",
      "43\n",
      "103\n",
      "1114\n",
      "130\n",
      "9\n",
      "42\n",
      "1708\n",
      "688\n",
      "2\n",
      "447\n",
      "255\n",
      "13\n",
      "2602\n",
      "13\n",
      "54\n",
      "108\n",
      "13\n",
      "122\n",
      "637\n",
      "1479\n",
      "359\n",
      "546\n",
      "13\n",
      "176\n",
      "2398\n",
      "2514\n",
      "63\n",
      "47\n",
      "9\n",
      "20\n",
      "4\n",
      "447\n",
      "208\n",
      "13\n",
      "68\n",
      "13\n",
      "1191\n",
      "13\n",
      "1613\n",
      "276\n",
      "13\n",
      "24\n",
      "58\n",
      "64\n",
      "9\n",
      "1287\n",
      "13\n",
      "384\n",
      "36\n",
      "22\n",
      "153\n",
      "1240\n",
      "67\n",
      "116\n",
      "397\n",
      "1613\n",
      "201\n",
      "13\n",
      "321\n",
      "1623\n",
      "21\n",
      "22\n",
      "4\n",
      "4\n",
      "22\n",
      "17\n",
      "2398\n",
      "20\n",
      "1095\n",
      "13\n",
      "14\n",
      "154\n",
      "488\n",
      "275\n",
      "99\n",
      "221\n",
      "342\n",
      "165\n",
      "107\n",
      "13\n",
      "11\n",
      "669\n",
      "366\n",
      "15\n",
      "480\n",
      "1293\n",
      "25\n",
      "169\n",
      "1758\n",
      "68\n",
      "167\n",
      "1177\n",
      "13\n",
      "175\n",
      "374\n",
      "99\n",
      "190\n",
      "255\n",
      "200\n",
      "800\n",
      "311\n",
      "58\n",
      "1353\n",
      "406\n",
      "13\n",
      "18\n",
      "260\n",
      "2\n",
      "2\n",
      "58\n",
      "13\n",
      "13\n",
      "14\n",
      "222\n",
      "165\n",
      "117\n",
      "1279\n",
      "14\n",
      "14\n",
      "18\n",
      "189\n",
      "158\n",
      "116\n",
      "13\n",
      "25\n",
      "663\n",
      "223\n",
      "165\n",
      "388\n",
      "326\n",
      "1229\n",
      "56\n",
      "11\n",
      "280\n",
      "200\n",
      "1498\n",
      "13\n",
      "14\n",
      "22\n",
      "99\n",
      "800\n",
      "11\n",
      "18\n",
      "386\n",
      "69\n",
      "140\n",
      "397\n",
      "17\n",
      "312\n",
      "283\n",
      "22\n",
      "1617\n",
      "111\n",
      "391\n",
      "2\n",
      "801\n",
      "173\n",
      "1430\n",
      "1153\n",
      "266\n",
      "118\n",
      "161\n",
      "150\n",
      "14\n",
      "280\n",
      "57\n",
      "170\n",
      "16\n",
      "300\n",
      "297\n",
      "99\n",
      "176\n",
      "16\n",
      "300\n",
      "204\n",
      "334\n",
      "141\n",
      "397\n",
      "153\n",
      "2255\n",
      "239\n",
      "3\n",
      "2505\n",
      "236\n",
      "280\n",
      "177\n",
      "13\n",
      "12\n",
      "388\n",
      "2\n",
      "200\n",
      "16\n",
      "37\n",
      "13\n",
      "99\n",
      "18\n",
      "50\n",
      "99\n",
      "13\n",
      "310\n",
      "200\n",
      "14\n",
      "24\n",
      "17\n",
      "200\n",
      "165\n",
      "733\n",
      "307\n",
      "337\n",
      "15\n",
      "300\n",
      "762\n",
      "2\n",
      "104\n",
      "255\n",
      "759\n",
      "461\n",
      "41\n",
      "58\n",
      "22\n",
      "63\n",
      "14\n",
      "23\n",
      "47\n",
      "4\n",
      "99\n",
      "241\n",
      "22\n",
      "281\n",
      "105\n",
      "6\n",
      "56\n",
      "154\n",
      "502\n",
      "1125\n",
      "231\n",
      "378\n",
      "4\n",
      "13\n",
      "201\n",
      "2213\n",
      "680\n",
      "24\n",
      "13\n",
      "2\n",
      "2301\n",
      "1286\n",
      "99\n",
      "201\n",
      "114\n",
      "42\n",
      "14\n",
      "15\n",
      "63\n",
      "451\n",
      "123\n",
      "395\n",
      "13\n",
      "68\n",
      "47\n",
      "16\n",
      "200\n",
      "58\n",
      "312\n",
      "560\n",
      "13\n",
      "17\n",
      "393\n",
      "207\n",
      "201\n",
      "2\n",
      "276\n",
      "311\n",
      "18\n",
      "111\n",
      "200\n",
      "56\n",
      "14\n",
      "2\n",
      "9\n",
      "67\n",
      "874\n",
      "9\n",
      "2\n",
      "307\n",
      "2262\n",
      "2436\n",
      "103\n",
      "68\n",
      "169\n",
      "126\n",
      "169\n",
      "14\n",
      "153\n",
      "144\n",
      "286\n",
      "204\n",
      "165\n",
      "106\n",
      "641\n",
      "248\n",
      "14\n",
      "58\n",
      "577\n",
      "2503\n",
      "13\n",
      "99\n",
      "2445\n",
      "2516\n",
      "15\n",
      "37\n",
      "108\n",
      "285\n",
      "589\n",
      "11\n",
      "13\n",
      "58\n",
      "705\n",
      "14\n",
      "99\n",
      "68\n",
      "286\n",
      "169\n",
      "94\n",
      "22\n",
      "2398\n",
      "14\n",
      "99\n",
      "1586\n",
      "1522\n",
      "107\n",
      "223\n",
      "196\n",
      "189\n",
      "16\n",
      "99\n",
      "144\n",
      "4\n",
      "28\n",
      "296\n",
      "529\n",
      "1132\n",
      "4\n",
      "41\n",
      "201\n",
      "753\n",
      "48\n",
      "480\n",
      "739\n",
      "196\n",
      "68\n",
      "42\n",
      "36\n",
      "1613\n",
      "14\n",
      "384\n",
      "200\n",
      "1244\n",
      "49\n",
      "790\n",
      "11\n",
      "68\n",
      "11\n",
      "67\n",
      "19\n",
      "227\n",
      "303\n",
      "200\n",
      "44\n",
      "62\n",
      "14\n",
      "13\n",
      "67\n",
      "391\n",
      "16\n",
      "319\n",
      "2\n",
      "67\n",
      "4\n",
      "23\n",
      "14\n",
      "51\n",
      "22\n",
      "1529\n",
      "9\n",
      "108\n",
      "20\n",
      "252\n",
      "1409\n",
      "64\n",
      "67\n",
      "220\n",
      "451\n",
      "402\n",
      "403\n",
      "23\n",
      "13\n",
      "153\n",
      "14\n",
      "311\n",
      "227\n",
      "2215\n",
      "201\n",
      "217\n",
      "1125\n",
      "200\n",
      "356\n",
      "99\n",
      "251\n",
      "69\n",
      "388\n",
      "68\n",
      "1315\n",
      "194\n",
      "355\n",
      "122\n",
      "41\n",
      "16\n",
      "7\n",
      "68\n",
      "155\n",
      "105\n",
      "160\n",
      "41\n",
      "46\n",
      "1250\n",
      "11\n",
      "48\n",
      "570\n",
      "4\n",
      "1404\n",
      "68\n",
      "14\n",
      "326\n",
      "349\n",
      "267\n",
      "16\n",
      "18\n",
      "286\n",
      "40\n",
      "58\n",
      "12\n",
      "9\n",
      "68\n",
      "68\n",
      "101\n",
      "67\n",
      "153\n",
      "47\n",
      "153\n",
      "2\n",
      "2398\n",
      "286\n",
      "294\n",
      "99\n",
      "14\n",
      "286\n",
      "99\n",
      "167\n",
      "1533\n",
      "1127\n",
      "24\n",
      "353\n",
      "179\n",
      "67\n",
      "99\n",
      "13\n",
      "11\n",
      "153\n",
      "65\n",
      "38\n",
      "165\n",
      "404\n",
      "40\n",
      "67\n",
      "169\n",
      "385\n",
      "9\n",
      "179\n",
      "155\n",
      "48\n",
      "3\n",
      "99\n",
      "25\n",
      "366\n",
      "200\n",
      "196\n",
      "188\n",
      "141\n",
      "2\n",
      "1251\n",
      "68\n",
      "1286\n",
      "200\n",
      "358\n",
      "67\n",
      "73\n",
      "2\n",
      "2\n",
      "13\n",
      "20\n",
      "153\n",
      "9\n",
      "13\n",
      "742\n",
      "148\n",
      "615\n",
      "11\n",
      "264\n",
      "80\n",
      "13\n",
      "13\n",
      "1081\n",
      "107\n",
      "150\n",
      "13\n",
      "169\n",
      "398\n",
      "17\n",
      "13\n",
      "22\n",
      "391\n",
      "310\n",
      "64\n",
      "43\n",
      "15\n",
      "4\n",
      "226\n",
      "14\n",
      "52\n",
      "622\n",
      "35\n",
      "789\n",
      "99\n",
      "3205\n",
      "275\n",
      "653\n",
      "153\n",
      "739\n",
      "280\n",
      "1494\n",
      "848\n",
      "47\n",
      "283\n",
      "17\n",
      "286\n",
      "815\n",
      "179\n",
      "13\n",
      "125\n",
      "181\n",
      "99\n",
      "13\n",
      "11\n",
      "1052\n",
      "266\n",
      "139\n",
      "36\n",
      "275\n",
      "13\n",
      "173\n",
      "171\n",
      "397\n",
      "16\n",
      "255\n",
      "67\n",
      "1091\n",
      "11\n",
      "13\n",
      "1411\n",
      "516\n",
      "311\n",
      "108\n",
      "22\n",
      "484\n",
      "99\n",
      "800\n",
      "49\n",
      "99\n",
      "99\n",
      "25\n",
      "574\n",
      "153\n",
      "57\n",
      "99\n",
      "22\n",
      "153\n",
      "99\n",
      "541\n",
      "287\n",
      "304\n",
      "173\n",
      "391\n",
      "13\n",
      "376\n",
      "265\n",
      "214\n",
      "186\n",
      "14\n",
      "140\n",
      "1219\n",
      "309\n",
      "309\n",
      "1531\n",
      "17\n",
      "348\n",
      "13\n",
      "577\n",
      "372\n",
      "195\n",
      "14\n",
      "143\n",
      "13\n",
      "99\n",
      "215\n",
      "22\n",
      "69\n",
      "99\n",
      "56\n",
      "2\n",
      "1220\n",
      "24\n",
      "99\n",
      "41\n",
      "238\n",
      "328\n",
      "175\n",
      "99\n",
      "56\n",
      "169\n",
      "743\n",
      "129\n",
      "2392\n",
      "11\n",
      "3134\n",
      "791\n",
      "2602\n",
      "9\n",
      "153\n",
      "58\n",
      "13\n",
      "25\n",
      "46\n",
      "230\n",
      "20\n",
      "9\n",
      "26\n",
      "17\n",
      "259\n",
      "13\n",
      "67\n",
      "13\n",
      "157\n",
      "110\n",
      "187\n",
      "12\n",
      "124\n",
      "99\n",
      "99\n",
      "153\n",
      "99\n",
      "18\n",
      "16\n",
      "393\n",
      "402\n",
      "166\n",
      "2\n",
      "402\n",
      "99\n",
      "14\n",
      "120\n",
      "14\n",
      "99\n",
      "189\n",
      "2482\n",
      "452\n",
      "239\n",
      "108\n",
      "449\n",
      "1620\n",
      "35\n",
      "23\n",
      "41\n",
      "2\n",
      "315\n",
      "11\n",
      "15\n",
      "274\n",
      "169\n",
      "23\n",
      "108\n",
      "129\n",
      "108\n",
      "18\n",
      "796\n",
      "153\n",
      "99\n",
      "2\n",
      "169\n",
      "101\n",
      "2\n",
      "108\n",
      "286\n",
      "13\n",
      "1082\n",
      "14\n",
      "242\n",
      "44\n",
      "68\n",
      "56\n",
      "386\n",
      "1499\n",
      "58\n",
      "23\n",
      "443\n",
      "217\n",
      "14\n",
      "1319\n",
      "94\n",
      "18\n",
      "23\n",
      "886\n",
      "23\n",
      "354\n",
      "59\n",
      "223\n",
      "99\n",
      "99\n",
      "155\n",
      "99\n",
      "43\n",
      "99\n",
      "99\n",
      "20\n",
      "402\n",
      "801\n",
      "341\n",
      "2224\n",
      "109\n",
      "300\n",
      "23\n",
      "153\n",
      "9\n",
      "1654\n",
      "99\n",
      "43\n",
      "156\n",
      "99\n",
      "125\n",
      "99\n",
      "99\n",
      "22\n",
      "13\n",
      "398\n",
      "170\n",
      "712\n",
      "392\n",
      "50\n",
      "647\n",
      "58\n",
      "310\n",
      "280\n",
      "311\n",
      "2\n",
      "114\n",
      "22\n",
      "78\n",
      "48\n",
      "68\n",
      "189\n",
      "58\n",
      "4\n",
      "1531\n",
      "1229\n",
      "622\n",
      "108\n",
      "67\n",
      "113\n",
      "118\n",
      "354\n",
      "480\n",
      "176\n",
      "60\n",
      "161\n",
      "13\n",
      "13\n",
      "109\n",
      "109\n",
      "312\n",
      "67\n",
      "275\n",
      "6\n",
      "9\n",
      "4\n",
      "88\n",
      "765\n",
      "19\n",
      "76\n",
      "2\n",
      "300\n",
      "228\n",
      "280\n",
      "19\n",
      "22\n",
      "22\n",
      "47\n",
      "13\n",
      "99\n",
      "102\n",
      "22\n",
      "67\n",
      "114\n",
      "11\n",
      "13\n",
      "705\n",
      "21\n",
      "44\n",
      "4\n",
      "220\n",
      "148\n",
      "800\n",
      "284\n",
      "276\n",
      "311\n",
      "183\n",
      "129\n",
      "227\n",
      "165\n",
      "44\n",
      "416\n",
      "13\n",
      "14\n",
      "365\n",
      "213\n",
      "219\n",
      "103\n",
      "22\n",
      "741\n",
      "275\n",
      "163\n",
      "13\n",
      "169\n",
      "169\n",
      "99\n",
      "108\n",
      "99\n",
      "56\n",
      "255\n",
      "120\n",
      "14\n",
      "17\n",
      "420\n",
      "326\n",
      "1499\n",
      "209\n",
      "108\n",
      "43\n",
      "135\n",
      "104\n",
      "23\n",
      "9\n",
      "191\n",
      "127\n",
      "99\n",
      "58\n",
      "189\n",
      "16\n",
      "483\n",
      "886\n",
      "56\n",
      "294\n",
      "16\n",
      "67\n",
      "4\n",
      "125\n",
      "40\n",
      "63\n",
      "17\n",
      "100\n",
      "69\n",
      "154\n",
      "109\n",
      "20\n",
      "210\n",
      "170\n",
      "169\n",
      "49\n",
      "14\n",
      "789\n",
      "16\n",
      "385\n",
      "99\n",
      "14\n",
      "11\n",
      "11\n",
      "13\n",
      "108\n",
      "13\n",
      "13\n",
      "81\n",
      "118\n",
      "741\n",
      "154\n",
      "13\n",
      "275\n",
      "365\n",
      "1533\n",
      "13\n",
      "169\n",
      "22\n",
      "99\n",
      "14\n",
      "1219\n",
      "169\n",
      "341\n",
      "19\n",
      "23\n",
      "80\n",
      "488\n",
      "9\n",
      "22\n",
      "14\n",
      "80\n",
      "4\n",
      "20\n",
      "255\n",
      "217\n",
      "959\n",
      "14\n",
      "199\n",
      "269\n",
      "248\n",
      "133\n",
      "46\n",
      "23\n",
      "4\n",
      "2\n",
      "58\n",
      "133\n",
      "81\n",
      "35\n",
      "427\n",
      "70\n",
      "656\n",
      "196\n",
      "18\n",
      "200\n",
      "427\n",
      "758\n",
      "37\n",
      "13\n",
      "4\n",
      "218\n",
      "113\n",
      "1203\n",
      "441\n",
      "99\n",
      "68\n",
      "2\n",
      "209\n",
      "391\n",
      "9\n",
      "2\n",
      "68\n",
      "2\n",
      "1522\n",
      "169\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "for index, row in final_data.iterrows():\n",
    "    print(sum(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the rows of the DataFrame\n",
    "data = pd.read_csv('sncb_prepared.csv', sep=';')\n",
    "print(len(data['events_sequence']))\n",
    "counter = 0\n",
    "for index, row in data.iterrows():\n",
    "    # check if the numbers 2708, 4026, 4068, 4066 appear is in the events_sequence\n",
    "    if '2708' in row['events_sequence'] and '4026' in row['events_sequence'] and '4068' in row['events_sequence'] and '4066' in row['events_sequence']:\n",
    "        counter += 1\n",
    "\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1]\n"
     ]
    }
   ],
   "source": [
    "#change '[1 , 1]' in real list [1, 1]\n",
    "string = '[1 , 1]'\n",
    "print(list(map(int, string.strip('[]').split(','))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
